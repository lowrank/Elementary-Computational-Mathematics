\chapter{Interpolation}
\label{Ch: 2-Int}

The interpolation solves problems of the following type:

\begin{quotation}
    Given a set of predefined functions $\cK$, find an element $f: \mathbb{I}\mapsto \bbR$ in $\cK$ such that
    $y_j = f(x_j)$ for all $j=0,\dots, n$.
\end{quotation}
Here, $\mathbb{I}$ denotes a finite or infinite interval such that $x_1,\dots x_n\in \mathbb{I}$. One of the important applications for interpolation is computer-assisted design (CAD), which is used extensively in the manufacturing industry. Generally speaking, the interpolation provides a closed form of the function to determine the value of $y$ where the parameter $x$ is not accessible.

\section{Polynomial Interpolation}
\label{Sec: 2-Pol-Int}
The polynomial interpolation considers the set $\cK = \Pi_m$, where the set $\Pi_m$ represents the polynomials of degree $ \le m$. We will seek for a polynomial $f(x)$ with the constraints that
\begin{equation}\nonumber
    \begin{cases}
        f\in \cK = \Pi_n,&\\
        f(x_k) = y_k &\text{ for } k = 0, 1,\dots, n.
    \end{cases}
\end{equation}
The points $x_k$ are called \emph{interpolation nodes}, if $m > n$ (resp. $m < n$), the problem is underdetermined (resp. overdetermined). For the case that $ m = n$, we have
\begin{theorem}
\label{Thm: 2-Uni-Exi}
    There exists a unique polynomial function $f\in \Pi_n$ such that $f(x_j) = y_j$ for $j=0,\dots, n$.
\end{theorem}
\begin{proof}
   \noindent \emph{Existence: }In order to construct the polynomial $f$, it is straightforward to consider the general form of polynomial $f(x) = \sum_{j=0}^n a_j x^j$, then we can formulate a linear system for the coefficients $a_j$, $j=0,\dots, n$, which is 
    \begin{equation*}
        \begin{pmatrix}
            1 & x_0 & x_0^2 & \dots & x_0^n \\
            1 & x_1 & x_1^2 & \dots & x_1^n \\
            \vdots & \vdots &\vdots & \ddots & \vdots\\
            1 & x_n & x_n^2 & \dots & x_n^n
        \end{pmatrix} \begin{pmatrix}
            a_0\\a_1\\\vdots\\a_n
        \end{pmatrix} = \begin{pmatrix}
            y_0\\y_1\\\vdots \\y_n
        \end{pmatrix}.
    \end{equation*}
    The matrix 
    $$V = \begin{pmatrix}
        1 & x_0 & x_0^2 & \dots & x_0^n \\
        1 & x_1 & x_1^2 & \dots & x_1^n \\
        \vdots & \vdots &\vdots & \ddots & \vdots\\
        1 & x_n & x_n^2 & \dots & x_n^n
    \end{pmatrix} $$
    is called \emph{Vandermonde matrix}.
    To determine the coefficients $a_j$, the matrix $V$ must be invertible. Its determinant can be computed (try to prove it) as
    \begin{eqnarray}
        \det(V) = \prod_{0\le i\le j\le n}(x_j - x_i).
    \end{eqnarray}
    When $\{x_j\}_{j=0}^n$ are distinct, the determinant is nonzero.

   \noindent \emph{Uniqueness: }
    Suppose there are two distinct polynomials $f, g\in \Pi_n$ satisfying the condition that $f(x_j) = g(x_j) = y_j$, then $f - g$ has $(n+1)$ roots $x_j$, $j=0, \dots, n$. If $f\neq g$, it is clear that $f-g\in\Pi_n$ has at most $n$ roots. Contradiction.
\end{proof}

In the above proof, the interpolation polynomial can be uniquely determined by solving the linear system 
$$\begin{pmatrix}
    1 & x_0 & x_0^2 & \dots & x_0^n \\
    1 & x_1 & x_1^2 & \dots & x_1^n \\
    \vdots & \vdots &\vdots & \ddots & \vdots\\
    1 & x_n & x_n^2 & \dots & x_n^n
\end{pmatrix} \begin{pmatrix}
    a_0\\a_1\\\vdots\\a_n
\end{pmatrix} = \begin{pmatrix}
    y_0\\y_1\\\vdots \\y_n
\end{pmatrix}.$$ 
However, it is generally easier to compute the polynomial $f$ with the \emph{Lagrange polynomial interpolation} (which is somewhat equivalent to computing the inverse of $V$).
\subsection{Lagrange Polynomial}
\begin{definition}
    For the given distinct $x_j$, $j = 0, 1, \dots, n$, the Lagrange polynomials $L_j\in\Pi_n$, $j=0,1,\dots, n$ are defined by 
    \begin{equation}
        L_j(x) = \prod_{s = 0, s\neq j}^n \frac{x - x_s}{x_j - x_s}, \quad j = 0, 1,\dots , n.
    \end{equation}
\end{definition}
It is clear that these polynomials satisfy the conditions that 
\begin{equation}
    L_j(x_k) = \delta_{jk} := \begin{cases}
        1&\text{for } k=j,\\
        0&\text{for } k\neq j.
    \end{cases}
\end{equation}
Therefore these polynomials are linearly independent, which form a basis of the $(n+ 1)$-dimensional space $\Pi_n$.
\begin{theorem}
\label{Thm: 2-Uni-Exi-Lag}
    The unique interpolating polynomial $f$ satisfying $f(x_j) = y_j$, $j=0,1,\dots, n$ can be represented by 
    \begin{equation}
        f(x) = \sum_{j=0}^n y_j L_j(x).
    \end{equation}
\end{theorem}
\begin{proof}
    It is straightforward to check the interpolation conditions are satisfied.
\end{proof}
\begin{remark}
    We introduce a preliminary procedure to compute the value of the interpolating polynomial $f$ at a point $x$. Let the constants $k_j$ and $q(x)$ be defined as 
    \begin{equation}
        k_j = \prod_{s= 0, s\neq j}\frac{1}{x_j - x_s},\quad q(x) = \prod_{j=0}^n (x - x_j),
    \end{equation}
    then 
    \begin{equation}
        f(x) = \sum_{j=0}^n y_j L_j(x) = q(x)  \sum_{j=0}^n k_j y_j \frac{1}{x - x_j}.
    \end{equation}
    One can first compute $k_j$ with $\cO(n^2)$ flops, then $f(x)$ can be computed with $\cO(n)$ flops. The advantage of the above scheme is the constants $k_j$ are independent of $y_j$, therefore evaluating another instance of the interpolating polynomial will not need to re-compute them. The disadvantage is that if we add a new node, the constants $k_j$ must be updated with an additional cost of $\cO(n)$ flops. Later we will see if Newton's form can overcome this issue.
\end{remark}
\subsection{Interpolation Error}
\label{SSec: 2-Int-Err}
When the data pairs $(x_j, y_j)$, $j=0,1,\dots, n$ are generated by a sufficiently smooth function $h(x)$, it is possible to quantify the error between the interpolating polynomial $f(x)$ and $h(x)$. 
\begin{theorem}
\label{Thm: 2-Int-err}
    Let $h: [a, b]\mapsto \bbR$ be a $(n+1)$-times differentiable function. If $f(x)\in\Pi_n$ is the interpolating polynomial that 
    $$f(x_j) = h(x_j),$$
    for $j=0,1,\dots, n$. Then, for each $\overline{x}\in [a, b]$, the error can be represented by 
    \begin{equation}
        h(\overline{x}) - f(\overline{x}) = \frac{\omega(\overline{x})}{(n+1)!} h^{(n+1)}(\xi),
    \end{equation}
    where $\xi = \xi(\overline{x})\in [a, b]$ and $\omega(x) = \prod_{j=0}^n (x - x_j)$.
\end{theorem}
\begin{proof}
    The proof is based on Rolle's Theorem. Select any $\overline{x}\in[a, b]$ such that $\omega(\overline{x})\neq 0$, then let 
    $$\psi(x) = h(x) - f(x) - k\omega (x),$$
    the constant $k$ is chosen such that $\psi(\overline{x}) = 0$. Then $\psi(x) = 0$ at $(n+2)$ points, 
    $$x_0, x_1, \dots, x_n, \overline{x}\in [a, b].$$
    By Rolle's Theorem, $\psi^{(n+1)}$ has at least one zero $\xi$ in $[a,b]$. Therefore,
    \begin{equation}
        \psi^{(n+1)}(\xi) = h^{(n+1)}(\xi) - 0 - k(n+1)! = 0.
    \end{equation}
\end{proof}

\begin{corollary}
    If $h(x)\in C^{\infty}([a, b])$ satisfies that $\max_{x\in[a,b]} |h^{(n)}(x)|\le M <\infty$ for all $n\ge 0$, then the interpolating polynomial approximates $h$ uniformly as the number of nodes $n\to \infty$.
\end{corollary}
\begin{proof}
    Since $|x -x_j|\le b-a$, the error is bounded by $\frac{(b-a)^{n+1}}{(n+1)!} M$, which converges to zero.
\end{proof}
It is interesting to think about the converse: under what condition does the interpolation error not vanish as the number of nodes tends to infinity? From Theorem~\ref{Thm: 2-Int-err}, the error depends on the sizes of three terms. 
\begin{enumerate}
    \item The bound of the $(n+1)$-th derivative, $\max_{x\in[a,b]}|h^{(n+1)}(x)|$. This could proliferate. For example, $h(x) = x^{-\sfrac{1}{2}}$ on $[\sfrac{1}{2}, \sfrac{3}{2}]$,
    \begin{equation}
         h^{(n+1)}(x) = \frac{(-1)^{n+1}}{2^{n+1}} (2n+1)!! x^{-(2n+3)/2}.
    \end{equation}
    \item The function $\omega(x) = \prod_{j=0}^n (x - x_j)$, such a product, could be large if $x$ and the nodes $x_j$ are not so close.
    \item The term $\frac{1}{(n+1)!}$, which decays fast.
\end{enumerate}
We can see that for the function $h(x) = x^{-\sfrac{1}{2}}$ on $[\sfrac{1}{2}, \sfrac{3}{2}]$, it is not trivial to show the interpolating polynomial could converge to $h$ anymore (although it is still true for certain choices of $x_j$, see Exercise~\ref{Prb: 2-Theo-2}). 

Next, we try to provide a better estimate of $\omega$ for the special choice: equally spaced nodes.  Let the nodes $x_j = a + j\Delta$, where $\Delta = \frac{b-a}{n}$. It is not difficult (prove it) to see $\omega(x)$ will be the worst if $x$ is located on the end sub-intervals, $[x_0, x_1]$ and $[x_{n-1}, x_n]$. Without loss of generality, we assume $x$ is located on $[x_0, x_1]$, then 
$$|x - x_j|\le j \Delta$$
for $j = 2, \dots, n$, which implies 
\begin{equation}
\label{EQ: MAX-OMEGA}
    |\omega(x)|\le \prod_{j=0}^n |x - x_j|\le n! \Delta^{n-1} \sup_{x\in [x_0, x_1]} |(x - x_0)(x-x_1)|= \frac{n!}{4} \frac{(b-a)^{n+1}}{n^{n+1}},
\end{equation}
Thus interpolation error is bounded by
$$  \|h  - f\|_{\infty} \le  \frac{\sup_{x\in[a,b]}|h^{(n+1)}(x)|}{4(n+1)} \frac{(b-a)^{n+1}}{n^{n+1}}.$$
Such an estimate is useful to derive uniform convergence. 
\begin{example}
    Consider $h(x) = x^{-1}$ on $[\sfrac{1}{2}, \sfrac{3}{2}]$. Then $h^{(n+1)}(x) = \frac{(n+1)!(-1)^{n+1}}{x^{n+2}}$, hence 
    \begin{equation}
        \frac{| h^{(n+1)}(x) |}{4(n+1)}  \frac{(b-a)^{n+1}}{n^{n+1}} \le \frac{1}{4n^{n+1}}\max_{x\in[\sfrac{1}{2}, \sfrac{3}{2}]} \left| \frac{1}{x^{n+2}} \right| = \frac{1}{2} \left(\frac{2}{n}\right)^{n+1},
    \end{equation}
    therefore, the interpolation error converges to zero exponentially. It is important to notice that the above method only works for intervals away from the origin. 
\end{example} 

\subsection{Runge's Phenomenon}
\label{SSec: 2-Run-Phe}
From the above discussion, we can see there is a possibility that $\max_{x\in[a,b]}|h^{n+1}(x)|$ grows much faster than $n^{n+2}$, which would lead to divergence. Hence, increasing the number of interpolation nodes (at least for equally spaced nodes) is not guaranteed for better approximation. The most famous example is the one made by Carl Runge. 
\begin{equation}
\label{EQ: RUNGE-EXAMPLE}
    h(x) = \frac{1}{1+x^2},\quad x\in [-5, 5].
\end{equation} 
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.7]{runge.png}
    \caption{Runge's phenomenon. Interpolation with 11 equally spaced nodes. }
\end{figure}
It can be shown that the interpolation will diverge at around $3.63$ as $n\to \infty$ and the maximum error $\max_{x\in[-5, 5]} |f_n(x) - h(x) |$ grows exponentially, where $f_n$ is the interpolating polynomial with $n+1$ equally spaced nodes. In the next section, we outline the idea of explaining the phenomenon briefly. 
\subsection{Interpolation Remainder Theory}
\label{SSec: 2-Int-Rem-The}
Let $f_n$ be the degree-$n$ polynomial interpolates $h$ at nodes $\{x_j\}_{j=0}^n$. If $h$ is analytic in a domain $T$ (possibly contains holes),  then the interpolation (Lagrange interpolant) can be written as 
\begin{eqnarray}
    f_n(z) = \sum_{j=0}^n \frac{\omega(z) h(x_j)}{(z - x_j) \omega'(x_j)}
\end{eqnarray}
Let $\psi(\xi; z) = \frac{(\omega(\xi) - \omega(z)) h(\xi)}{(\xi - z) \omega(\xi)}$, then by the Residue theorem for simple poles, if $z \neq x_j$,  
\begin{equation}
    \frac{1}{2\pi i}\int_{\partial T} \psi(\xi; z) d\xi = \sum_{j=0}^n \mathrm{Res}(\psi, x_j) = \sum_{j=0}^n \frac{(\omega(x_j) - \omega(z)) h(x_j)}{(x_j - z)\omega'(x_j)} = f_n(z),
\end{equation}
which implies that 
\begin{equation}
    h(z) - f_n(z) = \frac{1}{2\pi i}\int_{\partial T} \frac{\omega(z) h(\xi)}{(\xi - z)\omega(\xi)} d\xi.
\end{equation}
The error analysis is mainly studying the behavior of $|\omega(z)|$ as $n\to \infty$.
\begin{lemma}
\label{Lem: 2-Ome-Lim}
    If $\{x_j\}_{j=0}^n$ are equispaced nodes over $[a, b]$, then 
    \begin{equation}
        \lim_{n\to\infty} |\omega(z)|^{\frac{1}{n+1}} = \exp\left(\frac{1}{b - a}\int_a^b \log|z-\xi| d\xi \right).
    \end{equation}
\end{lemma}
\begin{proof}
    Taking $\log$ on $|\omega|^{\frac{1}{n+1}}$, then 
    \begin{equation}
        \log |\omega|^{\frac{1}{n+1}} = \frac{1}{n+1}\sum_{j=0}^n \log |z - x_j|\to \frac{1}{b-a}\int_a^b \log|z - \xi| d\xi. 
    \end{equation}
\end{proof}
Let $\sigma_n(z):= |\omega(z)|^{\frac{1}{n+1}}$ and define the contour $C_{\rho} = \{z\in \bbC\mid \sigma_n(z) = \rho\}$. These level sets are concentric closed curves about the midpoint of $[a,b]$.
\begin{lemma}
\label{Lem: 2-Ana-Uni-Con}
    Suppose the interpolation nodes $\{x_j\}_{j=0}^n$ are enclosed by $C_{\rho}$ and $h$ is analytic inside $C_{\rho}$. Let $z\in C_{\rho'}$ be such that $\rho'<\rho$, then $f_n\to h$ uniformly as $n\to\infty$. 
\end{lemma}
\begin{proof}
    Using the maximum modulus principle, the analytic function $h - f_n$ must attain its maximum modulus at the boundary $C_{\rho}$, thus 
    \begin{equation}
        |h(z) - f_n(z)| = \frac{1}{2\pi}\sup_{z\in C_{\rho'}} \left|\int_{C_{\rho}} \frac{\omega(z)}{\omega(\xi)} \frac{h(\xi)}{(\xi - z)} d\xi\right| \le C(\rho, \rho') \sup_{\xi\in C_{\rho}} \frac{|\omega(z)|}{|\omega(\xi)|},
    \end{equation}
    where $C(\rho, \rho')$ is a positive constant independent of $n$. For $n$ sufficiently large, we can find $0 < \delta < \frac{1}{3}(\rho - \rho')$ sufficiently small such that 
    \begin{equation}
        \sup_{\xi\in C_{\rho}} \frac{|\omega(z)|}{|\omega(\xi)|} \le \left|\frac{\rho' + \delta}{\rho - \delta}\right|^{n+1} \to 0\; \text{ as }n\to \infty. 
    \end{equation}
\end{proof}
When $h$ is not analytic inside $C_{\rho}$, let us consider a generic situation in which there exist isolated simple poles $z_k\in C_{\rho_k}$, $k\in [m]$ with $\rho_k < \rho$, then we select a contour $C_{\rho'}$ that $\rho_k <\rho'<\rho$ for all $k$. For $z\in C_{\rho'}$, we have
\begin{equation}
\begin{aligned}
    h(z) - f_n(z) &= \frac{1}{2\pi i} \int_{C_{\rho} - \bigcup_{k=1}^m \Gamma_k} \frac{\omega(z)h(\xi)}{(\xi - z) \omega(\xi)} d\xi \\
    &= \frac{1}{2\pi i} \int_{C_{\rho}} \frac{\omega(z)h(\xi)}{(\xi - z) \omega(\xi)} d\xi - \frac{1}{2\pi i}\sum_{k=1}^m \int_{\Gamma_k}  \frac{\omega(z)h(\xi)}{(\xi - z) \omega(\xi)} d\xi, 
\end{aligned}
\end{equation}
where $\Gamma_k$ is a small path surrounding $z_k$. The first term can be estimated using the lemma~\ref{Lem: 2-Ana-Uni-Con} whose limit goes to zero as $n\to \infty$. The second term is the summation
\begin{equation}
    \sum_{k=1}^m \frac{\omega(z)}{\omega(z_k)}\frac{\mathrm{Res}(h, z_k)}{z_k - z}.
\end{equation}
Since for sufficiently large $n$, we can find $0<\delta<\min_{k\in[m]}\frac{1}{3}(\rho'-\rho_k)$, 
\begin{equation}
    \left|\frac{\omega(z)}{\omega(z_k)}\right|^{\frac{1}{n+1}} = \frac{\sigma_n(z)}{\sigma_n(z_k)} > \min_{k\in [m]}\frac{\rho'-\delta}{\rho_k + \delta} > 1.
\end{equation}
Define the unique set $\cU = \{u_1, u_2,\cdots, u_l\}$ that $u_1 < u_2<\cdots < u_l$ which consists of all distinct values from $\{ \rho_1, \rho_2, \cdots,  \rho_m\}$, then the summation can be decomposed into $l$ groups:
\begin{equation}
 \sum_{k=1}^m \frac{\omega(z)}{\omega(z_k)}\frac{\mathrm{Res}(h, z_k)}{z_k - z} = \sum_{s=1}^l \sum_{\substack{k\in [m] \\ \rho_k = u_s}} \frac{\omega(z)}{\omega(z_k)}\frac{\mathrm{Res}(h, z_k)}{z_k - z}.
\end{equation}
As $n\to\infty$, if any of the groups does not vanish, then the whole summation must blow up as $n\to\infty$ (why?). Otherwise, the limiting summation should vanish, which violates the maximum modulus principle. 

In Runge's example~\eqref{EQ: RUNGE-EXAMPLE}, the simple poles $\pm i$ are on the contour $C_{\rho}$ which intersects the real line at $x_c\approx 3.6334$. Therefore, for $|x| < x_c$, the interpolation $f_n$ uniformly converges to $h$ and diverges once $|x| > x_c$. See Figure~\ref{fig:contour-curve}.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.65]{Figures/contour-curve.png}
    \caption{Contour $C_{\rho}$ with $\rho \approx 2.46879$, which passes through the simple pole $i$.}
    \label{fig:contour-curve}
\end{figure}
There exist better choices of interpolation nodes to prevent such a phenomenon. We will discuss this topic in Section~\ref{Sec: 2-Che-Int}. 

\subsection{Chebyshev Interpolation}
\label{Sec: 2-Che-Int}
The Chebyshev interpolation aims to minimize the bound of the interpolation error. The bound of $\omega(x)$ depends only on the choice of the nodes, so a natural question is: What kind of interpolation nodes will \emph{minimize} 
$\max_{x\in [a, b]} \prod_{j=0}^n |x-x_j|$. We first restrict our analysis to the interval $[a, b] = [-1,1]$ for simplicity, the general case will be discussed later.
 
\begin{example}
    When $n = 1$, $\omega(x) = (x - x_0)(x - x_1)$, this function changes sign over the subintervals $[-1, x_0)$, $(x_0, x_1)$, $(x_1, 1]$, then we can compute the maximum of $|\omega(x)|$ on these subintervals. Therefore, we need to solve 
    \begin{equation}
        \min_{x_0, x_1\in [-1,1]}\max((1 + x_0)(1 + x_1), \frac{(x_1-x_0)^2}{4}, (1 - x_0)(1 - x_1) ),
    \end{equation}
    while we can observe that 
    \begin{equation}
       \frac{1}{2} (1 + x_0)(1 + x_1) +  \frac{(x_1-x_0)^2}{4} + \frac{1}{2}(1 - x_0)(1 - x_1) = 1 + \frac{(x_0 + x_1)^2}{4}\ge 1
    \end{equation}
    holds for any choice of $x_0, x_1$, which means the maximum is at least $\sfrac{1}{2}$, it occurs when all terms are equal and $x_0 + x_1 = 0$. Hence $x_0 = -\sfrac{\sqrt{2}}{2}, x_1 = \sfrac{\sqrt{2}}{2}$.
\end{example}

\begin{definition}
    The Chebyshev polynomials of the first kind are defined by: 
    $$T_k(x) = \cos (k\arccos x),\quad x\in[-1,1].$$
\end{definition}
\begin{theorem}
\label{Thm: 2-Che-Pro}
    The Chebyshev polynomial satisfies the following: 
    \begin{enumerate}
        \item $T_k(\cos\theta) = \cos k\theta, \quad \theta\in [0, \pi]$.
        \item $T_0 \equiv 1$, $T_1(x) = x$ and 
        $$T_{k+1}(x) = 2 x T_{k}(x) - T_{k-1}(x), \quad k\ge 1.$$
        \item $\max_{x\in[-1,1]} |T_k(x)| = 1$.
        \item The leading coefficient of $T_k(x)$ is $2^{k-1}$.
        \item $T_k$ has a total of $(k+1)$ extrema $s_j = \cos(\frac{j\pi}{k}), j = 0, 1,\dots, n$ in the interval $[-1,1]$ such that $T_k(s_j) = (-1)^j$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    The first three statements are straightforward after replacing the variable $x = \cos\theta$. The fourth statement is an immediate result, with induction through the recursion formula 
    $T_{k+1}(x) = 2 x T_k(x) - T_{k-1}$. The last statement is trivial.
\end{proof}
More importantly, the Chebyshev polynomial has the following optimality property. 
\begin{theorem}
\label{Thm: 2-Che-opt}
    The optimal choice of interpolation nodes $\{x_j\}_{j=0}^{n}$ that minimize $\max |\omega(x)|$ is the extrema of Chebyshev polynomial $T_{{n+1}}$.
    \begin{equation}\label{EQ: CHEBY}
        \min_{x_j\in[-1,1]} \max_{x\in[-1,1]} |\omega(x)| =   \max_{x\in[-1,1]} \frac{1}{2^n}|T_{n+1}(x)|  = \frac{1}{2^n}
    \end{equation}
\end{theorem}
\begin{proof}
    Let the roots of $T_{n+1}(x)$ be $z_0, z_1, \dots, z_{n}\in [-1, 1]$, then we can write 
    $$T_{n+1} = 2^{n}(x - z_0)(x-z_1)\dots (x - z_{n}),$$
    therefore $\frac{1}{2^n} T_{n+1}(x)$ is a polynomial with leading coefficient as $1$. Since $\max_{x\in[-1,1]} |T_{n+1}(x)| = 1$, it is clear that $\max_{x\in [-1,1]} \frac{1}{2^n}|T_{n+1}(x)| = \frac{1}{2^n}$, which is the second equality in~\eqref{EQ: CHEBY}. For the first equality, we try to prove it by contradiction. Let $x_0, x_1, \dots, x_n\in [-1, 1]$, such that 
    $$\max_{x\in[-1,1]}|\omega(x)| < \frac{1}{2^n},$$
    then we define the polynomial $\psi(x) = \frac{1}{2^n}T_{n+1}(x)- \omega(x)$, its degree is at most $n$ due to cancellation, therefore at most have $n$ zeros. On the other side, because $\frac{1}{2^n}T_{n+1}(s_j) = \frac{1}{2^n}(-1)^j$ achieves the extrema at $s_j = \cos(\frac{j\pi}{n+1})$, $j=0,\dots, (n+1)$, the polynomial $\psi(s_j)$ must share the same sign of $\frac{1}{2^n}T_{n+1}(s_j)$. This means $\psi(x)$ changes sign $(n+1)$ times, hence $(n+1)$ zeros. It is a contradiction.
\end{proof}
\begin{definition}
    The interpolation nodes $z_j = \cos(\frac{(2j+1)\pi}{2(n+1)})$, $j = 0, 1, \dots, n$ are called ``\textit{Chebyshev nodes}''. These nodes are the zeros of Chebyshev polynomial $T_{n+1}$.
\end{definition}
We can generalize the above theorem to any interval $[a, b]$. One can define the affine transformation $\phi$ mapping $[-1,1]$ to $[a, b]$ by $\phi(x) = \frac{1}{2} (a + b + (b-a)x)$. It is not difficult to prove the following.
\begin{corollary}
\label{Cor: 2-Che-err-any}
    The optimal choice of interpolation nodes that minimize $\max |\omega(x)|$ on $[a, b]$ are, $\phi(z_j)$ and 
   $$\min_{x_j\in [a, b]} \max_{x\in [a, b]} |\omega(x)| = \frac{(b-a)^{n+1}}{2\cdot 4^n}.$$
   This bound is much smaller than the bound for equally spaced nodes.
\end{corollary}

\subsection{Stability of Polynomial Interpolation}
\label{SSec: 2-Sta-Pol-Int}
Suppose there is some perturbation of the data $\tilde{y}_j = y_j + \eps_j$ at the interpolation node $x_j$. Let $\tilde{f}_n(x)$ and $f_n(x)$ be the interpolating polynomials on perturbed data and original data. Then, with Lagrange polynomials, 
\begin{equation}
    \begin{aligned}
        |f_n(x) - \tilde{f_n}(x)| &= |\sum_{j=0}^n (y_j - \tilde{y}_j) L_j(x)| \\
        &\le \left(\max_{j} |\eps_j|\right) \sum_{j=0}^n |L_j(x)|.
    \end{aligned}
\end{equation}
Here $\lambda_n(x) := \sum_{j=0}^n |L_j(x)|$ is the \emph{Lebesgue function}. It is a piecewise polynomial. Its maximum $\Lambda_n$ is the \emph{Lebesgue constant} and only depends on the choice of interpolation nodes. 
For the equally spaced nodes, this Lebesgue constant grows exponentially. In fact,~\cite{turetskii1940bounding} proved the following sharp result. 
\begin{lemma}
    \label{Lem: 2-Leb-Con}
    Let $\{x_j\}_{j=0}^n$ be equispaced nodes on $[0, 1]$, then the Lebesgue constant 
    \begin{equation}
      \Lambda_n  =  \frac{2^{n+1}}{e n \log n} \left(1 + o(1)\right), \quad n\to\infty. 
    \end{equation}
\end{lemma}
\begin{proof}
    Assume $n\ge 3$, we prove the lower bound by construction. Let $f(x) = e^{i n\pi x}$ and define $p_n(x) = \sum_{j=0}^{n} f(x_j) L_j(x)$ as the interpolation polynomial at nodes $x_j = j \Delta$, $\Delta = \sfrac{1}{n}$, then $$|p_n(x)| = \left| \sum_{j=0}^{n} (-1)^j L_j(x) \right| \le \Lambda_n.$$
    Let $\delta_{+}$ be the forward difference operator
    \begin{equation}
       \delta_{+} p_n(x) := p_n(x+\sfrac{1}{n}) - p_n(x),
    \end{equation}
    then $ p_n(x_j) =  (1+\delta_{+})^j p_n(0)$ for $j=0,1,\cdots, n$, which means the interpolation polynomial is
    \begin{equation}
    \label{EQ: NEWTON-FORM-EQUI}
       p_n(x) = \sum_{k=0}^n \binom{nx}{k} \delta_+^k p_n(0) = \sum_{k=0}^n \binom{nx}{k} \delta_+^k f(0),
    \end{equation}
    which is exactly the Newton form at the equispaced nodes. See Section~\ref{SSec: 2-New-For}. Because $\delta_+ f(x) = (e^{i\pi} - 1)f(x) = -2 f(x)$ acts as a multiplicative operator, therefore 
    \begin{equation}
        p_n(x) = \sum_{k=0}^n \binom{nx}{k}  (-2)^k.
    \end{equation}
    Let $x_{\ast} = \frac{1}{n \log n}$ and set $\mu = n x_{\ast}\in(0, \sfrac{1}{\log(3)})$. Then, for $k\ge 1$, we have 
    \begin{equation}
        \frac{ \binom{\mu}{k}  (-2)^k  }{ \binom{\mu}{k + 1}  (-2)^{k + 1} } = \frac{k+1}{2(k - \mu)} > \frac{1}{2}.
    \end{equation}
    Therefore, 
    \begin{equation}
    \begin{aligned}
         \Lambda_n &\ge  |p_n(x_{\ast})| \ge \binom{\mu}{n}  (-2)^n \left(2 - \frac{1}{2^n}\right) - 1 \\
         &=\exp\left( \log\mu - \log n + \sum_{k=1}^{n-1} \log\left(1 - \frac{\mu}{k}\right) \right) (2^{n+1} - 1) - 1.
    \end{aligned}
    \end{equation}
    Denote the positive constant $C = 2(1 - \sfrac{1}{\log(3)})$ and notice $\log(1 - x) \ge -x - \frac{x^2}{C}$ for all $x\in (0, \sfrac{1}{\log(3)})$, then 
    \begin{equation}
    \begin{aligned}
        \Lambda_n &\ge \frac{\mu}{n} \exp\left(- \mu\sum_{k=1}^{n-1}\frac{1}{k}\right) \exp\left(-\frac{\mu^2}{C} \sum_{k=1}^{n-1}\frac{1}{k^2}\right)(2^{n+1} - 1) - 1 \\
        & \ge \frac{\mu}{n} \exp(-\mu (\log n + \gamma))\exp\left(-\frac{\pi^2\mu^2}{6C}\right)(2^{n+1} - 1) - 1 \\
        & =\frac{2^{n+1}}{e n \log n} \left(1 + o(1)\right).
    \end{aligned}
    \end{equation}
    We notice that if $\|f\|_{\infty}\le 1$,  then $\|\delta_{+} f\|_{\infty}\le 2$, using~\eqref{EQ: NEWTON-FORM-EQUI}, the upper bound of $\Lambda_n$ can be estimated by
    \begin{equation}
        \Lambda_n \le \sup_{x\in (0, 1)}\sum_{k=0}^n \left|\binom{nx}{k}\right| 2^k.
    \end{equation}
    By symmetry, it is sufficient to consider $x\le \sfrac{1}{2}$, otherwise, a similar bound can be derived with the backward difference operator $\delta_{-}$. Hence, 
    \begin{equation}
    \begin{aligned}
        \Lambda_n &\le \sum_{k=0}^{\lceil \sfrac{n}{2} \rceil} \binom{\lceil \sfrac{n}{2} \rceil}{k} 2^k + \sup_{z\in (0, \sfrac{n}{2})}  \sum_{k=\lceil \sfrac{n}{2} \rceil + 1}^{n} \binom{z}{k} 2^k \\
        &= 3^{ \lceil \sfrac{n}{2} \rceil } + \sum_{k = \lceil\sfrac{n}{2} \rceil + 1}^n  \frac{2^k}{e k \log k}\left(1 + o(1)\right),
    \end{aligned}
    \end{equation}
    where the following inequalities are applied:  
    \begin{equation}
        \begin{aligned}
            &\left|\binom{z}{k}\right|\le \binom{\lceil \sfrac{n}{2} \rceil}{k},\quad \forall z,k\le \lceil \sfrac{n}{2} \rceil
        \end{aligned}
    \end{equation} 
    and $\forall k \ge \lceil\sfrac{n}{2} \rceil + 1$, 
    \begin{equation}
        \begin{aligned}
            \sup_{z\in (0, \sfrac{n}{2})}\left|\binom{z}{k}\right| &=  \sup_{z\in (0, \sfrac{n}{2})} \frac{\sin(\pi(z - l))}{\pi k!} \Gamma(z+1)\Gamma(k - z) \\
            &=\sup_{z\in (0, 1)} \frac{\sin(\pi(z - l))}{\pi k!} \Gamma(z+1)\Gamma(k - z) \quad (\text{by comparison})\\&\le \sup_{z\in (0, 1)}\frac{z}{k}\left(\frac{z+1}{k - z}\right)^z \quad (\text{by Gautschi's inequality})\\
            &= \frac{1}{e k \log k}\left(1 + o(1)\right).
        \end{aligned}
    \end{equation}
    Therefore,  
    \begin{equation}
        \Lambda_n \le 3^{\lceil \sfrac{n}{2}\rceil}+  \frac{1}{e \log n} \left( \sum_{k=\lceil \sfrac{n}{2}\rceil+1}^n \frac{2^k}{k}\right) (1 + o(1)) =  \frac{2^{n+1}}{e n \log n}  (1 + o(1)).
    \end{equation}
\end{proof}
For the general case, it has been proved by Paul Erd{\"{o}}s (1964) that for any set of interpolation nodes, 
\begin{equation}
    \Lambda_n > \frac{2}{\pi}\log(n+1) + \frac{1}{2},\quad  n\ge 0.
\end{equation}
As the number of nodes $n\to \infty$, $\Lambda_n \to \infty$. This leads to the result of Faber that, for any choice of nodes, there exists a continuous function not able to be approximated by the interpolating polynomial. The Chebyshev nodes are almost optimal, in the sense that 
\begin{equation}
    \Lambda_{n, \textrm{Chebyshev}} < \frac{2}{\pi}\log(n+1) + 1.
\end{equation}
The set of nodes that minimizes $\Lambda_n$ is difficult to compute. A slightly better set of nodes than Chebyshev nodes are the \emph{ extended Chebyshev nodes}:
\begin{equation}
    \tilde{x}_j = \frac{\cos\left(\frac{2j+1}{2(n+1)\pi}\right)}{\cos\left(\frac{\pi}{2(n+1)}\right)}.
\end{equation}
\subsection{Newton Form}
\label{SSec: 2-New-For}
The Newton form is useful when we dynamically add interpolation nodes. Consider the following scenario: we already have found an interpolation polynomial $f_k$ through the data $(x_0, y_0)$, $(x_1, y_1)$,$\dots$, $(x_k, y_k)$, then if an addition pair $(x_{k+1}, y_{k+1})$ is provided, how can we effectively transform $f_k$ to $f_{k+1}$? If we write, 
$$f_{k+1}(x) = f_k(x) + c_{k+1} (x - x_0)(x - x_1)\dots (x - x_{k}), $$
then $f_{k+1}(x_j) = f_k(x_j)$, $j = 0, 1,\dots, k$. Therefore, we only need to take care of the equality $f_{k+1}(x_{k+1}) = y_{k+1}$, which means that 
\begin{equation}\label{EQ: CK}
    c_{k+1} = \frac{y_{k+1} - f_k(x_{k+1})}{\prod_{j=0}^k (x_{k+1} - x_j)}.
\end{equation}
Such an inductive procedure produces the Newton form: 
\begin{equation}\label{EQ: NEWTON}
    f_n(x) = c_0 + c_1 ( x - x_0) + c_2 (x - x_0)(x - x_1)+\dots+c_{n}(x-x_0)\dots (x - x_{n-1}).
\end{equation}
where the constant $c_j$ depends on $x_0, x_1, \dots, x_{j}$ only. Polynomials $\prod_{j=0}^k (x - x_j)$ are called \emph{Newton polynomials}. When the coefficients $c_k$ are known, the Newton form~\eqref{EQ: NEWTON} can be evaluated by the famous ``Horner's scheme'' (see Exercise~\ref{Prb: 1-Theo-3}), which is
\begin{equation}\label{EQ: HORNER}
    f_n(x) = c_0 + (x-x_0)(c_1 + (x-x_1)(c_2 + (x-x_2)(c_3 + \dots))),
\end{equation}
the evaluation order starts from the innermost part $c_n (x -x_{n-1})$. This formulation has a complexity of $3n$ flops. 

\begin{remark}
    The computation of $c_k$ is not cheap from~\eqref{EQ: CK}. A naive algorithm with Horner's scheme roughly takes $\sfrac{5n^2}{2}+\cO(n)$ flops to compute all coefficients. The ``divided differences'' is a better way to compute $c_k$.
\end{remark}
\begin{definition}
    Let the interpolation nodes be $\{x_0, x_1, \dots, x_n\}$, the ``divided differences'' are defined recursively as follows (the square bracket is used to distinguish from the usual bracket): 
    \begin{equation}
        \begin{aligned}
            f[x_j] &:= f(x_j),\\
        f[x_{j}, \dots, x_{j+k}] &:= \frac{f[x_{j+1},\dots, x_{j+k}] - f[x_j,\dots, x_{j+k-1}]}{x_{j+k} - x_{j}},
        \end{aligned}
    \end{equation}
    where $0\le j, k\le n$ and $j+k\le n$.
\end{definition}
The following example graph helps to understand the relationships among the divided differences. 

\begin{tcolorbox}
    \begin{equation}\label{EQ: ALG NEWTON}
    \begin{aligned}
        f[x_0] &               &\\ 
               &\searrow       &\\ 
        f[x_1] &\to f[x_0, x_1]& \\
               &\searrow        &\searrow&\\ 
        f[x_2] & \to f[x_1, x_2]&\to& f[x_0, x_1, x_2]\\
               &\searrow        &\searrow& &\searrow&\\ 
        f[x_3] &\to f[x_2, x_3] &\to& f[x_1, x_2, x_3] &\to & f[x_0, x_1, x_2, x_3]\\
               &\searrow        &\searrow & &\searrow&  &\searrow &\\ 
        f[x_4] & \to f[x_3, x_4]&\to& f[x_2, x_3, x_4] &\to & f[x_1, x_2, x_3, x_4] &\to&  f[x_0, x_1, x_2, x_3, x_4] 
    \end{aligned}
\end{equation}
\end{tcolorbox}
Calculating all the divided differences requires $\sfrac{3n^2}{2} +\cO(n)$ flops. The following theorem is the main statement for the Newton form.
\begin{theorem}
\label{Thm: 2-New-for}
    The interpolation polynomial $f_n$ in Newton form is given by, 
    \begin{equation}
        f_n(x) = f[x_0] + f[x_0, x_1](x-x_0) + \dots + f[x_0, \dots, x_n](x - x_0)(x - x_1)\dots (x - x_{n-1}).
    \end{equation}
    In other words, $c_k = f[x_0, \dots, x_k]$.
\end{theorem}
\begin{proof}
    We prove this by induction. Assume that the statement is true for $n$ and the interpolation node and the corresponding values  $(x_i, f[x_i])$, $i=0,1,\dots, n$. For a new node and value $(x_{n+1}, f[x_{n+1}])$, it is known from~\eqref{EQ: CK} that $c_{n+1}$ is the coefficient of the leading power. Let $g_n$ be the interpolation polynomial in Newton form through nodes $(x_i, f[x_i])$, $i=1,2,\dots, n+1$, then 
    $$\psi(x)  := g_n(x)(x - x_0) - f_n(x)(x - x_{n+1})$$
    satisfies that $\psi(x_j) = f[x_j](x_{n+1} - x_0)$ for $0\le j\le {n+1}$. Therefore,
    \begin{equation}
        f_{n+1}(x) = \frac{g_n(x)(x - x_0) - f_n(x)(x - x_{n+1})}{x_{n+1} - x_0}.
    \end{equation}
    The leading power's coefficient is then 
    \begin{equation}
        \frac{f[x_1, \dots, x_{n+1}] - f[x_0, \dots, x_n]}{x_{n+1} - x_0} = f[x_0, x_1,\dots, x_{n+1}].
    \end{equation}
\end{proof}
\begin{remark}
    The divided difference $f[x_j, \dots, x_{j+k}]$ is the coefficient of leading power of the interpolating polynomial through $(x_j, f[x_j]), \dots, (x_{j+k}, f[x_{j+k}])$. It can be shown that 
    $$f[x_j, \dots, x_{j+k}] = \frac{1}{k!}f^{(k)}(\xi)$$
    for some $\xi\in [a, b]$. See Exercise~\ref{Prb: 2-Theo-4}.
\end{remark}
\begin{remark}
\label{Rem: 2-New-err}
    The error estimate can be derived by
    \begin{equation}
        f(x) - f_n(x) = f[x_0, x_1, \dots, x_n, x] (x-x_0)\dots (x - x_n).
    \end{equation}
\end{remark}
\begin{remark}
    The Newton form~\eqref{EQ: NEWTON} does not require distinct nodes. The divided difference can be defined as a limit for repeated nodes:
    \begin{equation}
        f[x_0, x_0] = \lim_{x_1 \to x_0} \frac{f[x_1] - f[x_0]}{x_1 - x_0} = f'(x_0).
    \end{equation}
    Moreover, using the Taylor expansion, $f[\underbrace{x_0,\dots, x_0}_{(k+1)\,\text{times}}] = \frac{1}{k!}f^{(k)}(x_0)$. However, the divided differences cannot be computed in such cases if the derivative values are not provided. We will discuss this scenario later in the Hermite interpolation polynomial; see Section~\ref{SSec: 2-Her-Pol-Int}.
\end{remark}
\begin{remark}
    The algorithm to compute the divided difference can be more efficient with a single column to store diagonal elements. $\leadsto$ represents the number that is not changing.
    \begin{tcolorbox}
        \begin{equation}
        \begin{aligned}
            \color{red}{f[x_0]} &    \leadsto  \color{green}{f[x_0]}          & \leadsto& \color{cyan}{f[x_0]} &\leadsto& \color{blue}{f[x_0]} &\leadsto& \color{black}{f[x_0]}\\ 
                   &\searrow       &\\ 
                   \color{red}{f[x_1]} &\to \color{green}{f[x_0, x_1]}&  \leadsto& \color{cyan}{f[x_0, x_1]}  &\leadsto& \color{blue}{f[x_0, x_1]} &\leadsto& \color{black}{f[x_0, x_1]}\\
                   &\searrow        &\searrow&\\ 
                   \color{red}{f[x_2]} & \to \color{green}{f[x_1, x_2]}&\to& \color{cyan}{f[x_0, x_1, x_2]} &\leadsto& \color{blue}{f[x_0, x_1, x_2]}&\leadsto& \color{black}{f[x_0, x_1, x_2]}\\
                   &\searrow        &\searrow& &\searrow&\\ 
                   \color{red}{f[x_3]} &\to \color{green}{f[x_2, x_3] }&\to& \color{cyan}{f[x_1, x_2, x_3]} &\to & \color{blue}{f[x_0, x_1, x_2, x_3]}&\leadsto& \color{black}{f[x_0, x_1, x_2, x_3]}\\
                   &\searrow        &\searrow & &\searrow&  &\searrow &\\ 
                   \color{red}{f[x_4]} & \to \color{green}{f[x_3, x_4]}&\to& \color{cyan}{f[x_2, x_3, x_4]} &\to & \color{blue}{f[x_1, x_2, x_3, x_4]} &\to&  f[x_0, x_1, x_2, x_3, x_4] 
        \end{aligned}
    \end{equation}
    \end{tcolorbox}
\end{remark}
% \subsection{Berstein Polynomial Interpolation}
% The Berstein basis polynomial is defined by 
% \begin{equation}
%     b_{\nu, n}(x) = \binom{n}{\nu} x^{\nu}(1-x)^{n-\nu},\quad  \nu = 0,\dots, n
% \end{equation}

\subsection{Hermite Polynomial Interpolation}
\label{SSec: 2-Her-Pol-Int}
The Lagrange polynomial interpolation only requires the values of the data function $h$ at each node. It can be generalized when the derivative values of $h$ are also available. 

Let the tuple $(h(x_j), h^{(1)}(x_j), \dots, h^{(m_j)}(x_j))$ be the provided derivative values at the interpolation node $x_j$, $j=0,\dots, n$ and $m_j\ge 0$. $N = \sum_{j=0}^n (m_j + 1)$ is the total number of constraints. It can be shown that there exists a unique polynomial $H_{N-1}\in \Pi_{N-1}$ satisfies 
$$H_{N-1}^{(k)}(x_j) = y_j^k:= h^{(k)}(x_j),\quad j=0,\dots, n,\quad 0\le k\le m_j.$$
This polynomial is called \emph{Hermite interpolation polynomial}. The idea to construct the Hermite interpolation polynomial borrows from the Lagrange polynomials, which is to find a basis $L_{jk}$ such that 
\begin{equation}\label{EQ: HERMITE CONSTRUCTION}
    \frac{d^p}{d x^p}L_{jk}(x_l) = \begin{cases}
        1, & l = j, k = p\\
        0, & \text{otherwise}.
    \end{cases}
\end{equation}
Once these polynomials are obtained, the Hermite interpolation is straightforward:
$$H_{N-1}(x) = \sum_{j=0}^n \sum_{k=0}^{m_j} y_j^k L_{jk}(x).$$
Its uniqueness can be concluded from the linear independence of the basis $L_{jk}$. However, the construction method in~\eqref{EQ: HERMITE CONSTRUCTION} is not the simplest. It is known that the Newton form~\eqref{EQ: NEWTON} works for repeated nodes as long as the diagram's diagonal~\eqref{EQ: ALG NEWTON} can be filled. Therefore, we can arrange the nodes 
$$\underbrace{x_0,\dots, x_0}_{(m_0+1)\,\text{times}}, \quad \underbrace{x_1,\dots, x_1}_{(m_1+1)\,\text{times}}, \quad \dots,\quad  \underbrace{x_n,\dots, x_n}_{(m_n+1)\,\text{times}}$$
In this way, all the necessary divided differences can be computed.
\begin{remark}
    The error estimate for Hermite polynomial interpolation will be the same as the Newton form, see Remark~\ref{Rem: 2-New-err}.
\end{remark}


\section{Trigonometric Interpolation}
\label{Sec: 2-Tri-Int}
Periodic functions occur in many applications, that is, $f(x + T) = f(x)$, $x\in \bbR$ for some $T > 0$. For example, a closed planar curve can be parameterized as a periodic function naturally. The polynomial interpolation does not suit periodic functions, this is because polynomials will eventually go to infinity as $x\to\infty$. The most used interpolation for the periodic function is the \emph{trigonometric polynomial interpolation}. In the following, we assume the period $T = 2\pi$ without loss of generality.



\subsection{Fourier Series}
\label{SSec: 2-Fou-Ser}
\begin{definition}
    For $n\ge 0$, we defined $F_n$ the space of trigonometric polynomials 
    \begin{equation}
        F_n := \{f(x) \mid f(x) = \frac{a_0}{2} + \sum_{k=1}^n a_k \cos kx + \sum_{k=1}^n b_k \sin kx,\; a_k, b_k\in\bbR\}.
    \end{equation}
    The coefficients $a_0,\dots, a_n$, $b_1,\dots, b_n$ can be also chosen as complex numbers. $f\in F_n$ is said to be of degree $n$ if $|a_n| + |b_n| > 0$.
\end{definition}
The concept of ``degree'' here can be validated by the addition theorem of trigonometric functions. For instance, if $f_1\in F_k$, $f_2\in F_l$, then $f_1 f_2 \in F_{k+l}$. In the next, we discuss the uniqueness of the interpolation with the trigonometric polynomial.
\begin{lemma}
\label{Lem: 2-Tri-pl-zer}
    A trigonometric polynomial $f\in F_n$ that has more than $2n$ zeros in $[0, 2\pi)$ must vanish identically.
\end{lemma}
\begin{proof}
    Rewrite the trigonometric function in the form of 
    \begin{equation}\label{EQ: COMPLEX}
        f_n(x) = \sum_{k=-n}^{n} \gamma_k e^{ik x}. 
    \end{equation}
    where $\gamma_0 = \frac{1}{2}a_0$ and $\gamma_{k} = \frac{1}{2}(a_k - ib_k)$ and $\gamma_{-k} = \frac{1}{2}(a_k + i b_k)$, $k=1,\dots, n$. Then substitute $z = e^{ix}$ and set 
    \begin{eqnarray}
        p(z) = \sum_{k = -n}^n \gamma_k z^{n + k}, 
    \end{eqnarray}
    one can rewrite $f_n(x) = z^{-n} p(z)$. If $f_n(x)$ has more than $2n$ zeros, then $p(z)$ has more than $2n$ zeros, which is a contradiction since $p(z)$ is a polynomial of degree $2n$.
\end{proof}
\begin{remark}
    Since $\sin nx\in F_n$ has $2n$ zeros $\frac{\pi j}{n}$, $j=0,\dots, 2n-1$, it means to uniquely determine a trigonometric polynomial in $F_n$, exactly $2n+1$ values are needed. This is also known as the ``Nyquist-Shanon sampling theorem''.
\end{remark}
A direct corollary is the linear independence of the functions $1$, $\cos k x$ and $\sin k x$, $k = 1, \dots n$, these $(2n+1)$ functions form a natural basis for the trigonometric polynomial space $F_n$. 
\begin{corollary}
\label{Cor: 2-Fou-bas-ind}
    The functions $1, \cos kx, \sin kx$, $k=1,\dots, n$ are linearly independent on $C [0, 2\pi]$, hence $F_n$ is a $(2n+1)$ dimensional space.
\end{corollary}
To determine the coefficients $a_k, b_k$ from $(2n+1)$ data pairs $(x_j, y_j)$, $j=0, \dots, 2n$. We follow the idea of Lagrange polynomials by creating the basis polynomial $l_k(x)$ such that
\begin{eqnarray}
    l_k(x_j) = \begin{cases}
        1, &j = k,\\
        0,&\text{otherwise}.
    \end{cases} 
\end{eqnarray}
\begin{remark}
    A natural idea is replacing $x - x_j$ in the Lagrange basis by $\sin(x - x_j)$ and produce something like 
$$\prod_{j=0, j\neq k}^{2n}\frac{\sin(x - x_j)}{\sin(x_k - x_j)}$$
but $\sin(x - x_j)$ has two roots on $[0, 2\pi)$, therefore we need to rescale it to $[0, \pi)$.
\end{remark}
\begin{theorem}
\label{Thm: 2-Tri-Lag}
    Let the basis trigonometric polynomial, 
    $$l_k(x) =\prod_{j=0, j\neq k}^{2n}\frac{\sin(\frac{x - x_j}{2})}{\sin(\frac{x_k - x_j}{2})} ,$$
    then the interpolation trigonometric polynomial is 
    \begin{eqnarray}
        f_n(x) = \sum_{k=0}^{2n} y_k l_k(x).
    \end{eqnarray}
\end{theorem}
\begin{proof}
    It remains to show $l_k\in F_n$. This can be seen by splitting $l_k$ into $n$ pairs, each pair takes the form of 
    \begin{eqnarray}
        \sin(\frac{x-x_0}{2})\sin(\frac{x-x_1}{2}) = \frac{1}{2}\cos\left( \frac{x_0 - x_1}{2}\right) - \frac{1}{2}\cos\left(\frac{2x - x_0 - x_1}{2}\right)\in F_1.
    \end{eqnarray}
\end{proof}
Computationally, we can reuse the previously known barycentric form, but there exist better methods. For simplicity, we consider the equal space nodes in the following (non-uniform nodes could achieve the same complexity though). 
\begin{eqnarray}
    x_j = \frac{2\pi j}{2n + 1}, \quad j = 0, \dots, 2n. 
\end{eqnarray}
We will try to locate the coefficients $\gamma_k$ in the complex form (see~\eqref{EQ: COMPLEX}) from the interpolation conditions.
\begin{equation}
    f_n(x_j) = y_j = \sum_{k=-n}^n \gamma_k e^{i k x_j}.
\end{equation}
Use the property of the functions $e^{ikx_j}$ that 
\begin{equation}
    \sum_{k=0}^{2n} e^{ik x_j} = \begin{cases}
        2n+1, & k = 0\\
        0, &\text{otherwise}.
    \end{cases}
\end{equation}
It is not difficult to derive 
\begin{equation}
    \sum_{j = 0}^{2n} y_j e^{-im x_j} = \sum_{k=-n}^{n} \gamma_k \sum_{j=0}^{2n} e^{i (k - m) x_j} = \gamma_{m} (2n+1).  
\end{equation}
Therefore, we can compute 
\begin{equation}\label{EQ: GAMMA}
    \gamma_m = \frac{1}{2n+1}  \sum_{j = 0}^{2n} y_j e^{-im x_j}.
\end{equation}
When the coefficients $\gamma_m$ are known, Horner's scheme can be employed to evaluate the trigonometric polynomial in $\cO(n)$ time complexity.  However, naive computing of all the coefficients $\gamma_k$ will cost $\cO(n^2)$ flops. The fast Fourier transform can reduce the time complexity to $\cO(n\log n)$.
\begin{remark}
    For an even number of equally spaced nodes $x_j = \frac{j\pi}{n}$, $0\le j\le 2n-1$, the basis $\sin( n x)$ equals to zero constantly. There are $2n$ coefficients to be determined only. We can derive a similar formula as~\eqref{EQ: GAMMA} by replacing $(2n+1)$ with $2n$.
\end{remark}

\subsection{Fast Fourier Transform}
\label{SSec: 2-Fas-Fou-Tra}
The discrete Fourier transform $\textup{\texttt{DFT}}$ of a vector $\ba = (a_0, \dots, a_{n-1})$ is to evaluate the following vector:
$$\textup{\texttt{DFT}}(\ba)_{k} := \frac{1}{n}\sum_{j=0}^{n-1} a_je^{-2\pi i jk/n},\quad k = 0,\dots, n-1.$$
This is the exact formula to compute the coefficients for the trigonometric interpolation polynomial. Such transform is most efficiently calculated through the fast Fourier transform (\texttt{fft}).
The fast Fourier transform exploits the symmetry in $e^{2\pi i j/n}$ when $n$ is the power of two using divide-and-conquer. Let $\omega = e^{-2\pi i/n}$ and define $c_k$ as 
\begin{equation}
    c_k = \frac{1}{n}\sum_{j=0}^{n-1} y_j\omega^{jk},\quad k = 0,\dots, n-1.
\end{equation}
Let $m = \sfrac{n}{2}\in \bbN$, then $\omega^n = 1$, $\omega^m = -1$. We can separate $c_k$ into two parts with even $j$ and odd $j$.
\begin{equation}\label{EQ: C_K}
    c_k = \frac{1}{2} A_k + \frac{1}{2} B_k \omega^k,\quad c_{k+m} = \frac{1}{2} A_k - \frac{1}{2} B_k \omega^k 
\end{equation}
where 
\begin{equation}\label{EQ: AK BK}
    \begin{aligned}
        A_k = \frac{1}{m} \sum_{j=0}^{m-1} y_{2j} (\omega^2)^{jk},\quad B_k = \frac{1}{m} \sum_{j=0}^{m-1} y_{2j+1} (\omega^2)^{jk},
    \end{aligned}
\end{equation}
both $A_k$ and $B_k$ are in the same form and similar to $c_k$, but with only half of the terms in summation. This implies a recursive algorithm. Suppose $A_k$ and $B_k$, $0\le k\le m-1$ can be computed with $f(m)$ operations each, then 
\begin{equation}
   f(n) =  f(2m) = 2 f(m) + 4m
\end{equation}
The second term includes $2m$ multiplications and $2m$ additions in~\eqref{EQ: C_K}. Therefore,
\begin{equation}
    \begin{aligned}
        f(n)& = 2f(\frac{n}{2}) + 2n \\
&= 4f(\frac{n}{4}) + 2 n + 2n \\ 
&=\dots \\
&= n f(1) + \underbrace{2n + \dots + 2n}_{\log_2 n \text{ times}} = 2n \log_2 n.
    \end{aligned}
\end{equation}
since $f(1) = 0$, no computation is needed in this case.  The \texttt{fft} is usually a standard routine in modern scientific computing software. 

\subsection{Interpolation Error of Trigonometric Polynomial}
\label{SSec: 2-Int-Err-Tri-Pol}
The $L^2$ error estimate will be discussed at a later point. This part only focuses on the $L^{\infty}$ error estimate. We will need the following lemma, the proof is left as an exercise (see Exercise~\ref{Prb: 2-Theo-6}).
\begin{lemma}
\label{Lem: 2-Dir-Ker}
    Let $g(x)$ be the truncated Fourier series of $f(x)$
     $$g(x) = \sum_{s = -n}^n \gamma_s e^{is x},\quad \gamma_s = \frac{1}{2\pi}\int_{0}^{2\pi} f(x) e^{-isx} dx$$
     then 
     \begin{equation}
         g(x) =\frac{1}{2\pi}  \int_{0}^{2\pi} D_n(x - y) f(y)   d y, 
     \end{equation}
     where $D_n(x)$ is the \emph{Dirichlet kernel}
     \begin{equation}
         D_n(x):=  \sum_{s=-n}^{n} e^{is x} = \frac{\sin((2n+1)x/2)}{\sin(x/2)}.
     \end{equation}
\end{lemma}
\begin{theorem}[Uniform convergence for H\"older continuous functions]
\label{Thm: 2-Uni-Conv-C2}
    If $f\in C^{0,\alpha}(\bbR)$ is a $2\pi$-period function, then the trigonometric interpolation polynomial with $2n+1$ equally spaced nodes converges uniformly as the number of nodes tends to infinity.
\end{theorem}
\begin{proof}
    Since $f\in C^{0,\alpha}(\bbR)$, there exists a constant $K > 0$ that $|f(x) - f(y) |\le K |x - y|^{\alpha}$. Let $g(x)$ be the truncated Fourier series
    $$g(x) = \sum_{s = -n}^n \gamma_s e^{is x},\quad \gamma_s = \frac{1}{2\pi}\int_{0}^{2\pi} f(x) e^{-isx} dx$$
   and denote $h(x) = f(x)- g(x) = \sum_{|s| > n} \gamma_s e^{isx}$ the reminder. The interpolation polynomial satisfies
    \begin{equation}
        \begin{aligned}
            f_n(x)& = \sum_{m=-n}^n \left(\frac{1}{2n+1}\sum_{j=0}^{2n} y_j e^{-im x_j}\right) e^{im x}  \\
             &=\sum_{m=-n}^n \left(\frac{1}{2n+1}\sum_{j=0}^{2n} ( g(x_j) + h(x_j) ) e^{-im x_j}\right) e^{im x} \\
             &= \sum_{m=-n}^n \left(\frac{1}{2n+1}\sum_{j=0}^{2n} ( \sum_{s = -n}^n \gamma_s  e^{is x_j }+ h(x_j) ) e^{-im x_j}\right) e^{im x} \\
             &=g(x)+ \sum_{m=-n}^n \left(\frac{1}{2n+1}\sum_{j=0}^{2n} h(x_j) e^{-im x_j}\right)e^{im x} \\
             &=g(x)+ \frac{1}{2n+1} \sum_{j=0}^{2n} h(x_j) \frac{\sin((2n+1)(x-x_j)/2)}{\sin((x-x_j)/2)}
        \end{aligned}
    \end{equation}
    Therefore,
    \begin{equation}\label{EQ: DIFF}
        |f - f_n| \le \|h\|_{\infty} \left( 1 + \frac{1}{2n+1} \sum_{j=0}^{2n} \left|\frac{\sin((2n+1)(x-x_j)/2)}{\sin((x-x_j)/2)}\right|\right).
    \end{equation}
    We first estimate 
    $$\sum_{j=0}^{2n} \left|\frac{\sin((2n+1)(x-x_j)/2)}{\sin((x-x_j)/2)}\right|$$
    Separate the nodes into two groups: The first group with $|x - x_j| < \frac{2\pi}{2n+1}$, the absolute value is bounded by $(2n+1)$, there are at most $3$ nodes lying in this region, thus the contribution of the first group is at most $\cO(n)$. The second group is $\pi\ge |x - x_j| \ge \frac{2\pi}{2n+1}$, and then one can estimate 
    \begin{equation}
        \left|\frac{\sin((2n+1)(x-x_j)/2)}{\sin((x-x_j)/2)}\right| \le \frac{\pi}{|x - x_j|},
    \end{equation}
    since $\pi \sin x \ge 2x $ for $0\le x\le \sfrac{\pi}{2}$. This implies the contribution from the second group is $\cO(n\log n)$, and the total contribution is bounded by $\cO(n\log n)$. Then~\eqref{EQ: DIFF} can be bounded by 
    \begin{equation}
        |f - f_n| \le C \|h\|_{\infty} \log n,
    \end{equation}
    where $C$ is an absolute constant. Next, we estimate $\|h\|_{\infty}$. Using Lemma~\ref{Lem: 2-Dir-Ker}, we can represent
    \begin{equation}
    \begin{aligned}
        h(x) &= \frac{1}{2\pi} \int_{-\pi}^{\pi} D_n(y) (f(x) - f(x - y) ) dy \\
        &= \frac{1}{2\pi} \sum_{k=-n}^{n} \int_{\frac{(2k-1)\pi}{2n+1}}^{\frac{(2k+1)\pi}{2n+1}} \frac{\sin((2n+1) y/2)}{\sin(y/2)}  (f(x) - f(x - y) ) dy.
    \end{aligned}
    \end{equation}
    Notice the cancellation property
    \begin{equation}
         \int_{\frac{(2k-1)\pi}{2n+1}}^{\frac{(2k+1)\pi}{2n+1}} \sin\left(\frac{(2n+1) y}{2}\right) dy = 0,
    \end{equation}
    then for any $y'\in \cI_k:= \left(\frac{(2k-1)\pi}{2n+1}, \frac{(2k+1)\pi}{2n+1}\right)$, we have 
    \begin{equation}
    \label{EQ: CANCELLATION F}
    \begin{aligned}
    &\left| \int_{\frac{(2k-1)\pi}{2n+1}}^{\frac{(2k+1)\pi}{2n+1}} \frac{\sin((2n+1) y/2)}{\sin(y/2)}  (f(x) - f(x - y) ) dy \right| \\
    &= \left| \int_{\frac{(2k-1)\pi}{2n+1}}^{\frac{(2k+1)\pi}{2n+1}} \sin((2n+1) y/2)  \left( \frac{(f(x) - f(x - y) )}{\sin(y/2)} - \frac{(f(x) - f(x - y') )}{\sin(y'/2)} \right) dy \right| \\
    &\le \frac{4}{2n+1}\sup_{y,y'\in \cI_k} \left| \frac{(f(x) - f(x - y) )}{\sin(y/2)} - \frac{(f(x) - f(x - y') )}{\sin(y'/2)} \right|\\
    &\le \frac{4K}{2n+1}\left(\frac{\frac{\pi}{2n+1}\left|\frac{(2|k|+1)\pi}{2n+1}\right|^{\alpha}+ \frac{(2|k|+1)\pi}{4n+2}\left|\frac{2\pi}{2n+1}\right|^{\alpha}}{\left|\frac{(2|k|-1)\pi}{4n+2}\right|^2}\right)\\
    &=\frac{8K\pi^{\alpha-1}}{(2n+1)^{\alpha}} \left( \frac{2(2|k|+1)^{\alpha}}{(2|k|-1)^2}+ 2^{\alpha}\frac{2|k|+1}{(2|k|-1)^2} \right),
    \end{aligned}
    \end{equation}
    Because $\left|\frac{2|k|-1}{2|k|+1}\right|\ge \frac{1}{3}$ for all $k\in\bbN$, we obtain the bound for $h$:
    \begin{equation}
    \begin{aligned}
        |h(x)| &\le  \frac{4K\pi^{\alpha-2}}{(2n+1)^{\alpha}} \sum_{k=-n}^{n}  \left( \frac{2(2|k|+1)^{\alpha}}{(2|k|-1)^2}+ 2^{\alpha}\frac{2|k|+1}{(2|k|-1)^2} \right)\\&
        \le  \frac{36K\pi^{\alpha-2}}{(2n+1)^{\alpha}} \sum_{k=-n}^{n}  \left( \frac{2}{(2|k|+1)^{2-\alpha}}+ \frac{ 2^{\alpha}}{(2|k|+1)} \right)\\
        &= \cO(n^{-\alpha} \log n).
    \end{aligned}
    \end{equation}
    Therefore $|f - f_n|=\cO( n^{-\alpha} |\log n|^2 )$.
\end{proof}
\begin{remark}
    The uniform convergence of trigonometric polynomial interpolation be further extended to general continuous functions with \emph{modulus of continuity} $\omega(f;\tau)$ satisfying $$\omega(f;\frac{1}{n})|\log n|^2 \to 0$$ as $n\to\infty$. The estimate of $\|h\|_{\infty}$ matches the work of Dunham Jackson (1913). More related topics are discussed in Chapter~\ref{Ch: 4-App}.
\end{remark}

\section{Spline Interpolation}
\label{Sec: 2-Spl-Int}
It has been seen that increasing the number of interpolation nodes will not always help improve the approximation. The spline interpolation is to conquer this issue by using the piecewise low-degree polynomials. 

\begin{definition}
    Let $x_0, \dots, x_n$ be the distinct nodes on $[a, b]$ such that $a = x_0 <\dots < x_n = b$. The piecewise defined function $s_k(x)$ on the interval $[a, b]$ is a spline of degree $k$ to the nodes if  
    \begin{equation}
        s_k|_{[x_j, x_{j+1}]} \in \Pi_k, \quad s_k\in C^{k-1}([a, b]).
    \end{equation}
    The spline function $s_k$ is $(k-1)$-times continuously differentiable and piecewise polynomial of degree $k$.
\end{definition}
Then the space of splines $s_k$ will be $(n + k)$ dimension: each interval has $(k+1)$ dimensions, each interface imposes $k$ constraints, therefore $n (k+ 1) - (n-1) k = n + k$ dimensions. This shows that to determine a spline on the nodes uniquely, we will require $n+1$ interpolation values and $k-1$ additional constraints. Usual choices are
\begin{enumerate}
    \item periodic splines. $s_k^{(m)}(a) = s_k^{(m)}(b)$ for $m = 0, 1, \dots, k-1$.
    \item natural splines. $s_k^{(l+j)}(a) = s_k^{(l+j)}(b) = 0$, $j = 0, 1,\dots, l-2$ and $k = 2l-1$ with $l\ge 2$.
\end{enumerate}
In the following, we discuss some useful examples of spline. 
\subsection{Linear Splines}
The linear splines are a special case of splines. It uses piecewise linear polynomials on each subinterval and does not impose any derivative continuity. Let $y_j$ be the interpolation values at nodes $x_j$, respectively. The interpolation has an explicit form:
\begin{equation}
    s_1(x) = y_{j-1} + \frac{x - x_{j-1}}{x_j - x_{j-1}} y_j
\end{equation}
on the interval $[x_{j-1}, x_j]$. It can be represented as a linear combination of the ``hat'' basis function $\theta_j(x)$, defined by
\begin{equation}
    \theta_j(x) = \begin{cases}
        \frac{x - x_{{j-1}}}{x_j - x_{j-1}}, & x\in [x_{j-1}, x_{j}],\quad 1\le j\le n\\
         \frac{x - x_{j+1}}{x_j - x_{j+1}}, & x\in [x_j, x_{j+1}],\quad 0\le j\le n-1\\
         0, & \text{otherwise}
    \end{cases}
\end{equation}
then $s_1(x)$ can be written as 
$$s_1(x) = \sum_{j=0}^n \theta_j(x) y_j.$$
The interpolation error can be derived directly from the previous theory for two interpolation nodes. Let $f\in C^2([a, b])$, then on $[x_{j-1}, x_j]$, the interpolation error is 
\begin{equation}
    \frac{1}{2!}f''(\xi) (x - x_{j-1})(x - x_j) \le \frac{1}{8} \|f''\|_{\infty} |x_j - x_{j-1}|^2.
\end{equation}
Therefore, the interpolation error on $[a, b]$ is $\frac{1}{8} \|f''\|_{\infty} h^2$, where $h = \max|x_j - x_{j-1}|$. Once $f''$ is not uniformly bounded or even $f'$ is not well-defined somewhere, e.g., $f\in C^{0,\alpha}[a, b]$, the interpolation error will be replaced by the modulus of continuity. On $x\in [x_j, x_{j+1}]$, we have 
\begin{equation}
\begin{aligned}
    |s_1(x) - f(x)| &= \left| \frac{ (x_{j+1} - x) y_j+ (x - x_j) y_{j+1}}{x_{j+1} - x_j} - f(x) \right| \\   
    &=  \left| \frac{ (x_{j+1} - x) (y_j - f(x)) + (x - x_j) (y_{j+1}-f(x))}{x_{j+1} - x_j}  \right| \\ &\le \omega(f; |x_{j+1} - x_j|),
\end{aligned}
\end{equation}
where $\omega(f;\tau)$ is the modulus of continuity of $f$.
\subsection{Cubic Splines}
The cubic splines are particularly important in practice. Let $a = x_0 < x_1 <\dots < x_n = b$, and the corresponding values are $y_j$, $j = 0, \dots, n$. The constraints for cubic splines are: piecewise polynomial of degree 3 and continuous second derivative.a Denote the interpolation spline as $s_3$, then $\dd{s_3}$ is a piecewise linear function. On the sub-interval $[x_{j-1}, x_j]$, it can be represented by 
\begin{equation}
    \dd{s_3}(x) = M_{j-1} \frac{x_j - x}{h_j} + M_j \frac{x - x_{j-1}}{h_j},\quad j = 1,\dots, n,
\end{equation}
where $h_j = x_j - x_{j-1}$, $M_j = \dd{s_3}(x_j)$. Integrating the above formula twice, 
\begin{equation}
    s_3(x) = M_{j-1} \frac{(x_j - x)^3}{6 h_j} + M_j \frac{(x - x_{j-1})^3}{6h_j} + A_j (x - x_{j-1}) + B_j
\end{equation}
The additional constants $A_j, B_j$ can be determined by imposing $f(x_{j-1}) = y_{j-1}$ and $f(x_j) = y_j$. That is 
\begin{equation}
    A_j = \frac{y_j - y_{j-1}}{h_j} - \frac{h_j}{6}(M_j - M_{j-1}),\quad 
    B_j = y_{j-1}-  M_{j-1} \frac{h_j^2}{6}.
\end{equation}
Now we will determine the constants $M_j$ using the first derivative's continuity. 
\begin{equation}
    s_3'(x_{j}^{-})  = s_3'(x_j^{+}), \quad j = 1,\dots, n-1.
\end{equation}
That is equivalent to $j=1,\dots, n-1$, 
\begin{equation}
\label{EQ: CUB-SPL-DER}
    \begin{aligned}
        s_3'(x_{j}^{-})  &= M_j\frac{h_j}{3} + M_{j-1}\frac{h_{j}}{6} + \frac{y_j - y_{j-1}}{h_j}  \\
        &= -M_j \frac{h_{j+1}}{3}  - M_{j+1}\frac{h_{j+1}}{6} + \frac{y_{j+1} - y_j}{h_{j+1}} = s_3'(x_j^{+}). 
    \end{aligned}
\end{equation}
We can write the corresponding equations into a tridiagonal linear system 
\begin{equation}
    \begin{pmatrix}
        \frac{h_1}{6} & \frac{h_1 + h_{2}}{3} & \frac{h_{2}}{6} & \\\\
        &\frac{h_2}{6} & \frac{h_2 + h_{3}}{3} & \frac{h_{3}}{6} & \\\\
        &&\ddots &  \ddots & \ddots & \\ \\
        &&& \frac{h_{n-1}}{6} & \frac{h_{n-1} + h_{n}}{3} & \frac{h_{n}}{6} &
    \end{pmatrix} \begin{pmatrix}
        M_0 \\\\M_1\\\\ \vdots\\\\ M_n
    \end{pmatrix} 
    = \begin{pmatrix}
        \frac{y_{2} - y_{1}}{h_{2}} - \frac{y_{1} - y_{0}}{h_1} \\\\
        \frac{y_{3} - y_{2}}{h_{3}} - \frac{y_{2} - y_{1}}{h_2} \\\\
        \vdots \\\\
        \frac{y_{n} - y_{n-1}}{h_{n}} - \frac{y_{n-1} - y_{n-2}}{h_{n-1}} 
    \end{pmatrix}
\end{equation}
In practice, the system will be rescaled for numerical stability. 
\begin{equation}\nonumber
    \begin{pmatrix}
        \frac{h_1}{2(h_1 + h_{2})} & 1 & \frac{h_{2}}{2(h_1 + h_{2})} & \\\\
        &\frac{h_2}{2(h_2 + h_{3})} & 1 & \frac{h_{3}}{2(h_2 + h_{3})} & \\\\
        &&\ddots &  \ddots & \ddots & \\ \\
        &&& \frac{h_{n-1}}{2(h_{n-1} + h_{n})} & 1 & \frac{h_{n}}{2(h_{n-1} + h_{n})} &
    \end{pmatrix} \begin{pmatrix}
        M_0 \\\\M_1\\\\ \vdots\\\\ M_n
    \end{pmatrix} 
    = \begin{pmatrix}
       d_1 \\\\
        d_2 \\\\
        \vdots \\\\
       d_{n-1}
    \end{pmatrix}
\end{equation}
where $d_j = \frac{3}{h_{j-1} + h_{j}}\left[ \frac{y_{j} - y_{j-1}}{h_{j}} - \frac{y_{j-1} - y_{j-2}}{h_{j-1}}\right]$. 
The above system still lacks 2 more constraints, since the matrix is of size $(n-1)\times (n+1)$. Then we can apply the periodic spline or natural spline conditions. For example, if the natural constraint is applied: $\dd{s_3}(a) = \dd{s_3}(b) = 0$. We should have two more equations: 
\begin{equation}
    M_{0} = M_{n} = 0.
\end{equation} 
Then we can simply ignore the first and last columns of the matrix (also $M_0$ and $M_n$). 
If the periodic constraint is imposed, then we can add two more constraints: $M_0 = M_{n}$ and 
$$-M_0 \frac{h_{1}}{3} - M_1 \frac{h_1}{6} + \frac{y_1 - y_0}{h_1} = M_n \frac{h_n}{3} + M_{n-1} \frac{h_n}{6} + \frac{y_n - y_{n-1}}{h_n}.$$
In both cases, the resulting linear system is still tridiagonal and the solution takes $\cO(n)$ time complexity with the Thomas algorithm. 

Another popular choice to complete the matrix is to impose the constraints in the similar form on $x_0$ and $x_n$: 
\begin{equation}
\label{EQ: EXTENSION}
    2M_0 + \frac{h_1}{h_0 + h_1} M_1 = d_0,\quad  \frac{h_n}{h_{n} + h_{n+1}} M_{n-1} + 2M_n = d_n,
\end{equation}
where $h_0 = h_{n+1} = 0$ and $d_0 = d_1$, $d_{n} = d_{n-1}$ are assumed. 
\begin{remark}
The Jacobi and Gauss-Seidel iterations are suitable for solving the system. The iteration will converge to system accuracy within $\cO(\log_2 \textup{u})$ iterations. Related topics are discussed in Chapter~\ref{Ch: 8-Mat-Com}.
\end{remark}
The error estimate for the cubic spline can be derived in a way similar to the Lagrange polynomial interpolation. The following result is attributed to Charles Hall (1968).
\begin{theorem}\label{THM: ERROR CUBIC SPLINE}
    Let $f\in C^4([a, b])$ and $a = x_0 < \dots < x_n = b$ be a set of nodes. Then the natural cubic spline $s_3$ interpolating $f$ satisfies 
    \begin{equation}
        \|f - s_3\|_{\infty} \le \frac{5}{384}\|f^{(4)}\|_{\infty} h^4,
    \end{equation}
    where $h = \max_j |x_j - x_{j-1}|$.
\end{theorem}
\begin{proof}
    Here we only state the rough idea to prove the error bound. Let $u(x)$ be the piecewise Hermite interpolation polynomial that 
    \begin{equation}
        u(x_j)=f(x_j), \quad u'(x_j) = f'(x_j),
    \end{equation}
    then one can estimate 
    \begin{equation}
        \max_{x\in [x_j, x_{j+1}]} |u-f|\le  \frac{1}{24}\|f^{(4)}\|_{\infty} (x - x_j)^2 (x - x_{j+1})^2 \le \frac{1}{384}\|f^{(4)}\|_{\infty} h^4.
    \end{equation}
    On the subinterval $[x_i, x_{i+1}]$, $s_3$ and $u$ are both cubic polynomial interpolations, thus 
    \begin{equation}
         u(x) - s_3(x) = \frac{(x - x_i)(x_{i+1} - x)}{(x_{i+1} - x_i)} \left( e'(x_i) \frac{x_{i+1} - x}{x_{i+1} - x_i}  - e'(x_{i+1})\frac{x - x_i}{x_{i+1} - x_i}\right), 
    \end{equation}
    where $e(x) = f(x) - s_3(x)$. 
    Therefore, 
    \begin{equation}
    \label{EQ: CUB-SPL-ERR-EST-KEY}
    \begin{aligned}
        \|f - s_3\|_{\infty} &\le  \|u - s_3\|_{\infty}  +  \|f - u\|_{\infty} \\
        &\le \frac{h}{4} \max_{0\le i\le n} |e'(x_i)| + \frac{1}{384}\|f^{(4)}\|_{\infty} h^4. 
    \end{aligned}
    \end{equation}
    Using~\eqref{EQ: CUB-SPL-DER}, we find that
\begin{equation}
    \frac{1}{h_j}\left[ 2 s'_3(x_j) + s'_3(x_{j-1}) \right] + \frac{1}{h_{j+1}}\left[2 s'_3(x_j) + s'_3(x_{j+1})\right] =\frac{3(y_j - y_{j-1})}{h_j^2} + \frac{3(y_{j+1} - y_j)}{h_{j+1}^2}, 
\end{equation}
Using Taylor expansion locally at $x_j$, there exist $\zeta\in (x_{j-1}, x_j)$ and $\xi\in (x_j, x_{j+1})$ that
\begin{equation}
    \frac{2 e'(x_j) + e'(x_{j-1})}{h_j} + \frac{2 e'(x_j) + e'(x_{j+1})}{h_{j+1}}= \frac{1}{24}\left[-h_j^2 f^{(4)}(\zeta)+ h_{j+1}^2 f^{(4)}(\xi)\right].
\end{equation}
Suppose 
$\max_{0 \le i\le n}|e'(x_i)|$ attains its maximum at node $x_{k}$, then 
\begin{equation}
    \left| \frac{2 e'(x_k) + e'(x_{k-1})}{h_j} + \frac{2 e'(x_k) + e'(x_{k+1})}{h_{k+1}} \right| \ge \frac{h_j+h_{j+1}}{h_j h_{j+1}} |e'(x_k)|.
\end{equation}
Therefore, by AM-GM inequality,
\begin{equation}
    \max_{0 \le i\le n}|e'(x_i)| \le \frac{h_{j}h_{j+1}}{24 (h_j + h_{j+1}) } (h_j^2 + h_{j+1}^2) \|f^{(4)}\|_{\infty} \le \frac{1}{24}h^3 \|f^{(4)}\|_{\infty}. 
\end{equation}
Finally, combined with the estimate~\eqref{EQ: CUB-SPL-ERR-EST-KEY} will arrive at the desired bound.
\end{proof}
\subsection{B-Spline Representation}
The definition of B-spline (short for basis spline) was initially proposed by~\cite{curry1947spline}. Let $\bt:= \{t_j\}_{j\in \cJ}$ be a nondecreasing sequence; the index set $\cJ$ consists of consecutive integers which may be finite or infinite. The sequence is called ``knots''. The B-splines are defined in the following recurrence relation. 
\begin{definition}[Cox–de Boor]
The B-splines of order one $B_{j, 1}$ consist of the characteristic functions of the knot intervals:
\begin{equation*}
    B_{j, 1}(x) = \begin{cases}
        1,\quad x\in [t_j, t_{j+1})\\
        0,\quad \text{ otherwise }
    \end{cases}
\end{equation*}
and for $k > 1$, the B-spline of order $k$ satisfies
    $$B_{j,k}(x) = \frac{x - t_j}{t_{j+k-1} - t_j} B_{j, k-1}(x) + \frac{t_{j+k} - x}{t_{j+k} - t_{j+1}} B_{j+1, k-1}(x).$$
\end{definition}
Using induction, we can show the following property of the B-spline (see Exercise~\ref{Prb: 2-B-SPL-NON-SUP}).
\begin{lemma}[Property I]
\label{Lem: 2-B-SPL-NON-SUP}
    Each spline $B_{j,k}$ is a non-negative piecewise polynomial of degree $k-1$ supported on $(t_j, t_{j+k})$.
\end{lemma}
\begin{lemma}[Property II]
\label{Lem: 2-B-SPL-PAR-UNI}
   For any order $k\ge 1$, the B-splines of order $k$ form a partition of unity:
   $$\sum_{j = 1}^n B_{j, k}(x) = 1,\quad x\in (t_{k}, t_{n+1}).$$
\end{lemma}
\begin{proof}
    Let us prove this by induction. For $k=1$, this is exactly the definition. Assume the conclusion for $k = l$, then for $x\in (t_{l+1}, t_{n+1})\subset (t_{l}, t_{n+1})$, since $(t_{l+1}, t_{n+1})\bigcap \supp B_{1, l} = \emptyset$ and $(t_{l+1}, t_{n+1}) \bigcap \supp B_{n+1, l} = \emptyset$, we have 
    \begin{equation}
    \begin{aligned}
        \sum_{j=1}^n B_{j, l+1}(x) &=   \sum_{j=1}^n  \frac{x - t_j}{t_{j+l} - t_j} B_{j, l}(x) +   \sum_{j=1}^n\frac{t_{j+l+1} - x}{t_{j+l+1} - t_{j+1}} B_{j+1, l}(x) \\
        &=  \sum_{j=1}^n  \frac{x - t_j}{t_{j+l} - t_j} B_{j, l}(x) +  \sum_{j=2}^{n+1} \frac{t_{j+l} - x}{t_{j+l} - t_{j}} B_{j, l}(x) \\
        &= \sum_{j = 1}^{n} B_{j, l}(x) - \frac{t_{l+1} - x}{t_{l+1} - t_1} B_{1, l}(x) +  \frac{t_{n+l +1} - x}{t_{n+l + 1} - t_{n+1}} B_{n+1, l}(x) \\
        &=1. 
    \end{aligned}
    \end{equation}
    As an extension of Lemma~\ref{Lem: 2-B-SPL-PAR-UNI}, we actually can show B-spline can represent any polynomials.
    \begin{lemma}[Marsden]
    For $k \ge 1$, the B-splines of order $k$ can represent the polynomial
    \begin{equation}
    \label{EQ: MARSDEN}
        (x - \mu)^{k-1} = \sum_{j=1}^n \phi_{jk}(\mu) B_{jk}(x),\quad x\in (t_{k}, t_{+1}), 
    \end{equation}
    where $\phi_{jk}(\mu) = \prod_{l=j+1}^{j+k-1} (t_{l} - \mu)$ if $k > 1$, otherwise $\phi_{jk}(\mu) = 1$.
    \end{lemma}
    \begin{proof}
    The equality can be proved by induction as well. For the induction step, by the recursive definition, we have
        \begin{equation}\nonumber
        \begin{aligned}
            \sum_{j=1}^n \phi_{j,l+1}(\mu) B_{j, l+1}(x) &= \sum_{j=1}^n  \frac{x - t_j}{t_{j+l} - t_j} \phi_{j,l+1}(\mu) B_{j, l}(x) +   \sum_{j=1}^n\frac{t_{j+l+1} - x}{t_{j+l+1} - t_{j+1}} \phi_{j,l+1}(\mu) B_{j+1, l}(x)\\
            &=  \sum_{j=1}^n  \frac{x - t_j}{t_{j+l} - t_j}(t_{j+l}-\mu) \phi_{j,l}(\mu) B_{j, l}(x) \\&\quad +  \sum_{j=2}^{n+1} \frac{t_{j+l} - x}{t_{j+l} - t_{j}} (t_{j-1+l}-\mu)\phi_{j-1, l}(\mu)  B_{j, l}(x) \\
            &=  \sum_{j=1}^n  \frac{x - t_j}{t_{j+l} - t_j}(t_{j+l}-\mu) \phi_{j,l}(\mu) B_{j, l}(x) \\&\quad +  \sum_{j=2}^{n+1} \frac{t_{j+l} - x}{t_{j+l} - t_{j}} (t_{j}-\mu)\phi_{j, l}(\mu)  B_{j, l}(x)\\
            &=  (x - \mu)  \sum_{j=1}^n\phi_{j,l}(\mu) B_{j, l}(x),\quad x\in (t_{l+1}, t_{n+1})\subset (t_{l}, t_{s+1}). 
        \end{aligned}
        \end{equation}
    \end{proof}
    The B-splines of order $k$ naturally define a linear subspace $\cS_{k,\bt}$:
    \begin{equation}
        \cS_{k,\bt} = \text{span} \left\{ B_{jk}(x) \mid j=1,\cdots, n\right\}.
    \end{equation}
    Then $\cS$ contains all polynomials of degree $\le k-1$. Indeed, differentiating~\eqref{EQ: MARSDEN} in $\mu$, one gets
    \begin{equation}
        \frac{(x - \mu)^{k - \nu}}{(k - \nu)!} = \sum_{j=1}^n\frac{(-1)^{\nu}\partial_{\mu}^{\nu} \phi_{jk}(\mu) B_{jk}(x)}{(k-1)!}, \quad x\in (t_{k}, t_{n+1}). 
    \end{equation}
    If a knot $t_i$ has a multiplicity of $m_i > 1$, that is, 
    $$t_{i-1} < \underbrace{t_i = t_{i+1} = \cdots = t_{i + m_{i} - 1} }_{\text{multiplicity } = m_i}< t_{i + m_i}, $$
    then let $\mu = t_i$, for any $1\le \nu \le m_i$, 
    \begin{equation}
        \frac{(x - t_i)^{k - \nu}}{(k - \nu)!} = \sum_{j= i - ( k -\nu)}^s \frac{(-1)^{\nu}\partial_{\mu}^{\nu} \phi_{jk}(t_i) B_{jk}(x)}{(k-1)!}, \quad x\in (t_{i + \nu-1}, t_{n+1}) = (t_i, t_{n+1}). 
    \end{equation}
    However, $\partial_{\mu}^{\nu} \phi_{jk}(t_i) = 0$ for $i-1 \ge j \ge i - (k-\nu)$, thus 
    \begin{equation}
         \frac{(x - t_i)^{k - \nu}}{(k - \nu)!} = \sum_{j= i}^n \frac{(-1)^{\nu}\partial_{\mu}^{\nu} \phi_{jk}(t_i) B_{jk}(x)}{(k-1)!}, \quad x\in (t_i, t_{n+1}). 
    \end{equation}
    We also notice $B_{jk}(x) = 0$ if $x\le t_i$ for all $j\ge i$, therefore, 
    \begin{equation}
        \frac{(x - t_i)^{k - \nu}_{+}}{(k - \nu)!} = \sum_{j= i}^n \frac{(-1)^{\nu}\partial_{\mu}^{\nu} \phi_{jk}(t_i) B_{jk}(x)}{(k-1)!}, \quad x\in (t_{k}, t_{n+1}).
    \end{equation}
    \end{proof}
    \begin{definition}
        Suppose $\bzeta = \{\zeta_i\}_{i=1}^{q+1}$ is an increasing sequence and denote by $\bm = \{m_i\}_{i=2}^q$ an integer nonnegative sequence that $m_i \le k$. Define $\Pi_{k, \bt, \bm}$ as the space of piecewise polynomials of degree at most $k-1$ with knots $\{\zeta_i\}_{i=1}^{q+1}$ and satisfying the additional jump condition $\forall f\in \Pi_{k, \bzeta, \bm}$ 
        \begin{equation}
            \partial^{\nu-1} f(\zeta_i^{+}) = \partial^{\nu-1} f(\zeta_i^{-}),\quad 1 \le \nu \le m_i.
        \end{equation}
    \end{definition}
    \begin{theorem}[Curry-Schoenberg]
        \label{Thm: 2-B-SPL-EQU-PP-SPA}
        Let the non-decreasing sequence $\bt$ be 
        \begin{equation}
            \underbrace{\zeta_1, \cdots, \zeta_1}_{\text{multiplicity } = k}, \underbrace{\zeta_2, \cdots, \zeta_2}_{\text{multiplicity } = k - m_2},\cdots, \underbrace{\zeta_{q}, \cdots, \zeta_q}_{\text{multiplicity } = k - m_n}, \underbrace{\zeta_{q+1}, \cdots, \zeta_{q+1}}_{\text{multiplicity } = k} 
        \end{equation}
        then on the interval $[t_{k}, t_{n+1}]$, $n = \text{card}(\bt) - k$, 
        $$\Pi_{k, \bzeta, \bm} = \cS_{k, \bt}.$$
    \end{theorem}
    \begin{proof}
        $\Pi_{k,\bzeta, \bm}$ is spanned by the basis functions 
        \begin{equation*}
            \begin{aligned}
                &(x - \mu)^{k-\nu}, \quad \nu = 1,\cdots, k; \\
                &(x - t_i)^{k - \nu}_{+},\quad i=2,\cdots, q,\quad 1\le \nu \le k - m_i.
            \end{aligned}
        \end{equation*}
       Therefore, $\Pi_{k,\bzeta, \bm}\subset \cS_{k, \bt}$. The dimension of $\cS_{k, \bt}$ is the number of B-splines $B_{jk}$, which is $\text{card}(\bt) - k = q k - \sum_{i=2}^n m_i$ which corresponds to the total number of bases of $\Pi_{k,\bzeta, \bm}$. 
    \end{proof}
From now on, we assume that the knot sequence $\bt$ is defined as Theorem~\ref{Thm: 2-B-SPL-EQU-PP-SPA}, where the first and last knots are repeated $k$ times, then the basic interval $[t_k, t_{n+1}]$ coincides with the range of knots. By the partition of unity (see Lemma~\ref{Lem: 2-B-SPL-PAR-UNI}), the spline function in $\cS_{k,\bt}$ is a strictly convex combination of the coefficients. In other words, the spline function is bounded from below and above by the coefficients nearby.
\begin{theorem}[Convex Hull]
    \label{Thm: 2-B-SPL-CON-NUM}
    Suppose $x \in (t_i, t_{i+1})$, then 
    $$ \min_{i+1-k\le j \le i} c_j \le  \sum_{j=i-k+1}^i c_j B_{jk}(x)  \le \max_{i+1-k\le j\le i} {c_j}.$$
\end{theorem}
\subsection{B-Spline Interpolation}
\label{SSec: 2-B-SPL-INT}
Let $n = \dim(\cS_{k, \bt})$ for the knots $\bt$. For the observation data $\{(x_i, y_i)\}_{i=1}^n$, the B-spline interpolation finds a spline $f(x) = \sum_{j=1}^n c_j B_{jk}$ that 
\begin{equation}
    \sum_{j=1}^n c_j B_{jk}(x_i) = y_i, \quad i = 1,\cdots, n.
\end{equation}
We first consider the criteria for the existence and uniqueness of the interpolation.
\begin{theorem}[Existence and Uniqueness]
Let $\{x_j\}_{j=1}^n$ be strictly increasing. The matrix $A:= (B_{jk}(x_i))_{ij}$ is non-singular if and only if $B_{ik}(x_i) \neq 0$ or equivalently $t_i < x_i < t_{i+k}$.
\end{theorem}
\begin{proof}
    The ``only if'' part is trivial.
\end{proof}
The following theorem shows that the B-spline representation is relatively stable for lower degrees. 
\begin{theorem}[Good Condition]
    \label{Thm: 2-B-SPL-GOO-CON}
    Given any knot sequence, let $f = \sum_{j=1}^n c_j B_{jk}$ on $[t_k, t_{n+1}]$, then 
    \begin{equation}
        \frac{1}{D_{k,\infty}} \max_{1\le j\le n} |c_j| \le \|f\|_{\infty} \le \max_{1\le j \le n} |c_{j}|,
    \end{equation}
    where the condition number $D_{k,\infty} \le k 2^{k - 1}$. 
\end{theorem}
\begin{proof}
    
\end{proof}
A precision characterization of the constant is conjectured as $D_{k,\infty} < c 2^{k}$, this is known as the ``De Boor's $2^k$ conjecture''~\cite{de1990exact}. Currently, the best-known result is $D_{k,\infty} \le k 2^{k-1}$ by~\cite{scherer1999new}. 






\section{Reproducing Kernel Hilbert Space}
\label{Sec: 2-Rep-Ker-Hil-Spa}
Let $H$ be a Hilbert space of functions defined on $\cX$, with inner product $\aver{\cdot, \cdot}$ such that the \emph{point evaluation map} $x: H\mapsto \bbC$ is bounded $\forall x\in \cX$. Then $H$ is called \emph{reproducing kernel Hilbert space} or RKHS for short. By Riesz's representation Theorem, there exists $\psi_x\in H$ that 
\begin{equation}
    f(x) = \aver{f, \psi_x},\quad \forall f\in H.
\end{equation}
The function $\cK(x, y):=\psi_x(y)$ is the \emph{reproducing kernel} and $\psi_x(\cdot)$ is the \emph{reproducing element} for point $x\in \cX$.
\begin{example}
    $L^2[0, 1]$ with the usual inner product is not an RKHS. However, the Sobolev space $W^{1, 2}[0, 1]$ with its usual norm is a RKHS by the embedding theorems. One can prove that any Sobolev spaces with embedding $C^{0}(\Omega)\subset\subset W^{k,p}(\Omega)$ will be an RKHS on $\Omega$.
\end{example}
The kernel $\cK(x, y):\cX\times \cX\to\bbC$ satisfies 
\begin{equation}
    \cK(x, y) = \aver{\psi_x, \psi_y}, 
\end{equation}
and the kernel is Hermitian and positive definite. Its converse is also true.
\begin{example}
    The positive definite $\cK(x, x)\ge 0$ is strict, that is, if $\cK(x, x) = 0$, then $\|\psi_x\|_{H} = 0$ which forces $\psi_x = 0$. Furthermore, the Cauchy-Schwartz inequality implies 
    \begin{equation}
        | \cK(x, y) |\le \sqrt{\cK(x, x)\cK(y, y)}. 
    \end{equation}
\end{example}
\begin{theorem}[Moore-Aronszajn]
    \label{Thm: 2-MOO-ARO}
    If $\cK:\cX\times \cX\to\bbC$ is a Hermitian, positive definite kernel, then there is a unique RKHS with $\cK$ as its reproducing kernel.
\end{theorem}
\begin{proof}
    We prove the RKHS is $H=\overline{\text{span}\{\cK(x, \cdot) \mid x\in \cX\}}$ equipped with the induced inner product $\aver{\cK(x, \cdot), \cK(y,\cdot)}_H = \cK(x, y)$. The verification is straightforward. For uniqueness, assume that $H'\supset H$ is another RKHS with kernel $\cK$ and inner product $\aver{\cdot, \cdot}_{H'}$. Then 
    \begin{equation}
        \cK(x, y) = \aver{\cK(x, \cdot), \cK(y,\cdot)}_H = \aver{\cK(x, \cdot), \cK(y,\cdot)}_{H'},
    \end{equation}
    which implies the inner products are identical on $H$. Finally, $\forall f\in H'$, write $f = f_H + f_{H^{\perp}}$, then 
    \begin{equation}
        f(x) = \aver{f, \cK(x, \cdot)}_{H'} = \aver{f_H, \cK(x, \cdot)}_{H'}  + \aver{f_{H^\perp}, \cK(x, \cdot)}_{H'} = f_H(x).
    \end{equation}
    Therefore, $H' = H$.
\end{proof}
\begin{theorem}
    Let $\cK_{H}:\cX\times \cX \to \bbC$ be the reproducing kernel for the Hilbert space $H$ and $\cK_{H'}: \cX\times \cX \to \bbC$ be the reproducing kernel for a closed subspace $H'\subset H$. Then $\forall f\in H$,  
    \begin{equation}
        f'(x) := \aver{f, \cK_{H'}(x, \cdot)} \in H',
    \end{equation}
    and $f'$ is the projection of $f$ onto $H'$.
\end{theorem}
\begin{proof}
    For any $f\in  H$, let $f = f_{H'} + f_{H'^{\perp}}$, where $f_{H'}$ is the projection of $f$ onto $H'$, then note $\cK_{H'}(x, \cdot)\in H'$,  
    \begin{equation}
        f'(x) = \aver{f_{H'} + (f - f_{H'}), \cK_{H'}(x, \cdot)} = \aver{f_{H'}, \cK_{H'}(x, \cdot)} = f_{H'}(x). 
    \end{equation}
    Therefore $f' = f_{H'}$. Moreover, by the definition, 
    \begin{equation}
        \cK'_{H}(z, x) = \aver{\cK_{H}(z, \cdot), \cK_{H'}(x, \cdot)} = \overline{ \aver{\cK_{H'}(x, \cdot), \cK_{H}(z, \cdot)} } = \overline{\cK_{H'}(x, z)} = \cK_{H'}(z, x) 
    \end{equation}
    which implies $\cK'_{H}=\cK_{H'}$ is the projection of $\cK_{H}$ onto $H'$.
\end{proof}
\begin{example}
    If $H = L^2(\cX, \mu)$ be the standard Lebesgue space with a probability measure $\mu$, then for any closed subspace $H'\subset H$, the projection of $f\in H$ onto $H'$ is
    \begin{equation}
        f'(x) = \aver{f, \cK_{H'}(x, \cdot)} = \int_{\cX} f(z) \cK_{H'}(x, z) d\mu(z). 
    \end{equation}
    Note that the Hilbert space $H$ itself may not be an RKHS, it only needs $H'$ to be an RKHS. 
\end{example}
\begin{example}
    If $\cL\in H^{\ast}$ is a bounded linear functional on the Hilbert space $H$, then by Rietz representation theorem, there exists $\ell\in H$ such that
    \begin{equation}
        \cL \cK(x, \cdot) = \aver{\cK(x, \cdot), \ell} = \overline{\aver{\ell, \cK(x,\cdot)}} = \overline{\ell(x)}.  
    \end{equation}
    Similarly, for any bounded linear operator $\cA: H\to H$ can be represented by the reproducing kernel in the sense 
    \begin{equation}
        (\cA f)(x) = \aver{\cA f, \cK(x, \cdot)} = \aver{f, \cA^{\ast} \cK(x,\cdot)}. 
    \end{equation}
\end{example}
\begin{theorem}
    If $H$ is a separable RKHS with orthonormal basis $\{\phi_i\}_{i\ge 1}$, then the reproducing kernel permits the absolutely convergent expansion
    \begin{equation}
        \cK(x, y) = \sum_{i=1}^{\infty} \phi_i(x) \overline{\phi_i(y)}, 
    \end{equation}
    which is independent of the choice of basis. (Note that separable Hilbert spaces have countably many orthonormal bases.)
\end{theorem}
\begin{proof}
     Let the subspace $H_n = \textrm{span}\{\phi_i\}_{i=1}^n$, then the reproducing kernel of $H_n$ is 
     \begin{equation*}
         \cK_n(x, y) = \sum_{i=1}^n \overline{\phi_i(x)} {\phi_n(y)},
     \end{equation*}
     since for every $f\in H_n$, $f(x) = \sum_{i=1}^n \aver{f, \phi_i} \phi_i(x)$, thus 
     \begin{equation*}
         \aver{f, \cK_n(x, \cdot)} = \sum_{i=1}^n \aver{f, \phi_i} \phi_i(x) = f(x),
     \end{equation*}
     which implies $\cK_n$ is the reproducing kernel on $H_n$. Then let $n\to\infty$, using the fact (see Exercise~\ref{Exer:2-Rep-Ker-Ine}) that 
     \begin{equation}
         \sum_{i=1}^{n} |\phi_i(x)|^2 = \cK_n(x, x) \le \cK(x, x).
     \end{equation}
     Then, the expansion converges absolutely by Cauchy-Schwartz inequality.
\end{proof}
\subsection{Interpolation in RKHS}
\label{SSec: 2-Int-RKH}
Let $H$ be an RKHS on the non-empty set $\cX$ with the reproducing kernel $\cK(x, y)$. Suppose that the data $\{(x_j, y_j)\}_{j=1}^m$ are generated by $  y_j = f(x_j) = \aver{f, \cK(x_j,\cdot)} $ for a unknown function $f\in H$. The interpolation finds a function $f^{\ast}$ such that 
\begin{equation}
    \aver{f^{\ast}, \cK(x_j,\cdot)} = y_j,\quad j=1,\cdots, m.
\end{equation}
The choice for $f^{\ast}$ may not be unique. In practice, the one with a minimal norm is preferred, that is, $f^{\ast}\in \textrm{span}\{ \cK(x_j,\cdot)\}_{j\in[m]}$.   

\begin{theorem}
    Suppose $\{\cK(x_j, \cdot)\}_{j\in [m]}$ are linearly independent. If there exists an element $h\in \textrm{span}\{ \cK(x_j,\cdot)\}_{j\in[m]}$ such that $h(x_j) = y_j$, then $\|h\|$ has the minimal norm.
\end{theorem}
\begin{proof}
    
\end{proof}
If the number of data $m\to\infty$, then the problem becomes related to the \emph{universal interpolation problem}. 
\begin{definition}
    Let $H$ be an RKHS with reproducing kernel $\cK:\cX\times\cX\to \bbC$. The sequence of points $\{x_i\}_{i\ge 1}\subset \cX$ is a universal interpolation sequence (UIS) if the interpolation problem of finding $f\in H$ 
    \begin{equation}
        f(x_i) = y_i,\quad i \in\bbN
    \end{equation}
    is solvable whenever $\sum_{i\ge 1} \frac{|y_i|^2}{\cK(x_i, x_i)} < \infty$. 
\end{definition}
\begin{lemma}
\label{Lem: 2-UIS}
    The sequence $\{x_i\}_{i\in\bbN}$ is a UIS if and only if the infinite Hermitian matrix $A$ with entries
    \begin{equation}
        A_{ij} = \frac{\cK(x_i, x_j)}{\sqrt{\cK(x_i, x_i)\cK(x_j, x_j)}}
    \end{equation}
    has all of the eigenvalues bounded away from zero. 
\end{lemma}
\begin{proof}
    
\end{proof}
\begin{corollary}
    Suppose $H$ is an RKHS on $\bbR^d$ and let the set $G\subset \bbR^d$ be a group. If the reproducing kernel satisfies $\cK(x, y) = \cK(x - y)$ and  
    $$\sum_{g\in G, g\neq 0}|\cK(g)| < \cK(0),$$
    then $G$ is a UIS.
\end{corollary}
\begin{proof}
    The matrix entry $A_{ij} = \cK(x_i - x_j)$. For $\forall x\in G$, $x - G$ traverses the group, therefore, for a fixed $j\in\bbN$, the summation
    \begin{equation}
        \sum_{i\neq j}|A_{ij}| = \sum_{g\in G, g\neq 0} |\cK(g)| < \cK(0) = A_{ii}. 
    \end{equation}
    Therefore the matrix $A$ is an $M$ matrix (See Chapter~\ref{Ch: 8-Mat-Com}). The eigenvalues are bounded away from zero by the Gershgorin circle theorem. The conclusion is directly from Lemma~\ref{Lem: 2-UIS}.
\end{proof}
\begin{example}[Dirichlet kernel]
    TODO: the subgroup is the circle group over $\{k\in\bbN\mid |k|\le n\}$. 
\end{example}
\subsection{Hardy Space \texorpdfstring{${\mathbb{H}}^2$}{}}
\label{SSec: 2-Har-Spa}
The Hardy space $\mathbb{H}^2$ refers to the holomorphic functions in the unit disk that 
\begin{equation}
    \sup_{0\le r <  1} \int_0^{2\pi} |f(r e^{i\theta})|^2 d\theta < \infty.
\end{equation}
With the usual expansion $f(z) = \sum_{n=0}^{\infty} c_n z^n$, this is equivalent to $\sum_{n=0}^{\infty} |c_n|^2 < \infty$. 
\section{Exercises}
The coding part and test cases are available on GitHub. 
\subsection{Theoretical Part}
\begin{problem}
    Prove Corollary~\ref{Cor: 2-Che-err-any}.
\end{problem}
\begin{problem}
    Prove there exists a constant $C > 0$ that the following bound holds. Compare this bound with~\eqref{EQ: MAX-OMEGA}.
    \begin{equation}
        |\omega(x)|\le \prod_{j=0}^n |x - x_j| \le \frac{n!}{C + \log n} \left|\frac{b-a}{n}\right|^{n+1}.
    \end{equation}
\end{problem}
\begin{problem}
\label{Prb: 2-Theo-2}
    Estimate the interpolation error for $e^x$ and $x^{-\sfrac{1}{2}}$ over the interval $[\frac{1}{2}, \frac{3}{2}]$ with equally spaced nodes and Chebyshev nodes, respectively.
\end{problem}
\begin{problem}
    Let $-1\le x_0 < x_1 <\dots < x_n \le 1$ be a set of interpolation nodes and $L_j(x)$ denotes the Lagrange basis polynomial at $x_j$. Prove if $x\in (x_i, x_{i+1})$, then 
    $$L_{i}(x) + L_{i+1}(x) \ge 1.$$
    Hint: Use Rolle's Theorem.
\end{problem}
\begin{problem}
    Prove the Berstein inequality  for trigonometric polynomials $t_n(\theta) = \sum_{k=-n}^n c_k e^{ik\theta}$.
    \begin{equation}\nonumber
        \|t_n'(\theta)\|_{\infty} \le n \|t_n(\theta)\|_{\infty},\quad \theta\in [0, 2\pi).
    \end{equation}
    and use it to prove the Markov inequality for any $p_n\in\Pi_n$ and $x\in [-1,1]$, 
    \begin{equation*}
        \|p_n'(x)\|_{\infty}\le n^2 \|p_n'(x)\|_{\infty}.
    \end{equation*}
\end{problem}
\begin{problem}
    Prove the contour $C_{\rho}$ for Chebyshev nodes on $[-1, 1]$ is an ellipse with foci at $\pm 1$. 
\end{problem}
\begin{problem}
\label{Prb: 2-Theo-4}
    Let the nodes $x_0, \dots, x_n\in[a,b]$ and $f\in C^{n+1}([a, b])$. For any given $x\in[a, b]$, prove there exists $\xi\in [a, b]$ such that the divided difference $$f[x_0, x_1, \dots, x_n, x] = \frac{1}{(n+1)!}f^{(n+1)}(\xi).$$
    Hint: {Construct the interpolation polynomial on nodes $x_0, \dots, x_n$ with Newton form first, then regard $x$ as the additional node by~\eqref{EQ: NEWTON}, finally recall the Theorem~\ref{Thm: 2-Int-err}.}
\end{problem}
\begin{problem}
    Show that the divided difference $$f[x_0, x_1,\dots, x_n] = f[x_{\pi(0)}, x_{\pi(1)},\dots, x_{\pi(n)}],$$ where $\pi\in S_{n+1}$ is any permutation.
\end{problem}
\begin{problem}
\label{Prb: 2-Theo-6}
    Prove Lemma~\ref{Lem: 2-Dir-Ker}.
\end{problem}
\begin{problem}[discrete circular convolution]
    Let two vectors $\ba = (a_0,\dots, a_{N-1})$ and $\bb = (b_0, \dots, b_{N-1})$ and we assume the convention that $a_{N + j} = a_j$ and $b_{N+j} = b_j$ to extend the vector to infinite size. The discrete circular convolution $\bc = (c_0,\dots, c_{N-1}) = \ba\ast \bb$ is defined by 
$$c_j = \sum_{l = 0}^{N-1} a_l b_{j - l}.$$
Prove that 
$$\textup{\texttt{DFT}}(\bc) = \textup{\texttt{DFT}}(\ba)\circ \textup{\texttt{DFT}}(\bb),$$
where $\circ$ is the Hadamard product or element-wise product.
\end{problem}
\begin{problem}[Minimimum norm property] Let $f\in C^2([a, b])$ and $s_3$ be the \underline{natural} cubic spline with interpolating $f$. Prove 
    \begin{equation}
        \int_a^b |\dd{s_3}(x)|^2 dx \le \int_{a}^b |f''(x)|^2 dx 
    \end{equation}
    The equality holds only when $f(x) = s_3(x)$ everywhere.
    Hint: One needs to prove 
    \begin{equation}
        \int_a^b (f''(x) - \dd{s_3}(x)) \dd{s_3}(x) dx = 0,
    \end{equation}
    then simply use the AM-GM inequality. The above equality can be proved using integration-by-parts twice (remember the interpolation conditions).
\end{problem}
\begin{problem}
    Let the following tridiagonal matrix (corresponding to natural cubic spline)
    $$A = \begin{pmatrix}
        1 & \frac{h_{2}}{2(h_1 + h_{2})} & \\\\
        \frac{h_2}{2(h_2 + h_{3})} & 1 & \frac{h_{3}}{2(h_2 + h_{3})} & \\\\
        &\ddots &\ddots &  \ddots & \\ \\
        &&\ddots &\ddots &  \frac{h_{n-2}}{2(h_{n-2} + h_{n-1})} & \\ \\
        &&& \frac{h_{n-1}}{2(h_{n-1} + h_{n})} & 1&
    \end{pmatrix} $$
    and denote $B = A - I$. Prove $\|B\|_{\infty} \le \frac{1}{2}$ and $\|A^{-1}\|_{\infty}\le 2$. 
\end{problem}
\begin{problem}
\label{Prb: 2-B-SPL-NON-SUP}
    Prove the Lemma~\ref{Lem: 2-B-SPL-NON-SUP}.
\end{problem}
\begin{problem}
\label{Exer:2-Rep-Ker-Ine}
    Let $H'$ be a closed subspace of the Hilbert space $H$. Show that the reproducing kernels $\cK_{H'}(x, x) \le \cK_{H}(x, x)$. 
\end{problem}
\begin{problem}
    Find the reproducing kernel for $W^{1,2}(\bbR)$. 
\end{problem}
\subsection{Computational Part}
\begin{problem}
    Implement the divided difference to compute the coefficients of the Newton form. You should call the Horner's scheme from the previous coding project. State the consequence when the length of $[a, b]$ is very small. 
\end{problem}
\begin{problem}
    Implement the Lagrange interpolation schemes on equispaced and (extended) Chebyshev nodes.
\end{problem}
\begin{problem}
    Implement a routine to compute Hermite interpolation with $m_j = 1$ for each $j = 0, \dots, n$. Estimate your routine's time complexity and space complexity. 
\end{problem}
\begin{problem}
    Implement the cubic spline interpolation with periodic and natural spline conditions. 
\end{problem}

\begin{problem}
     Implement discrete Fourier transform using a recursive approach (assume the total number of nodes is a power of two).
\end{problem}

\begin{problem}
    Implement the B-splines $B_{jk}$ for the knots mentioned in Theorem~\ref{Thm: 2-B-SPL-EQU-PP-SPA}.
\end{problem}

% \bibliography{563}
\nocite{jackson1912approximation, jackson1913accuracy, erdHos1964problems,erdos1958problems,de1978practical,prenter2008splines,trefethen1991two}
\bibliographystyle{apalike}
\bibliography{chap2}
