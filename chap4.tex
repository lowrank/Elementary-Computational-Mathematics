\chapter{Approximation}
\label{Ch: 4-App}
The approximation solves the problem 
$$\min_{p\in P}\|f - p\|_{\cX}$$
which aims to select the function $p\in P$ in a specific set with the minimum distance under a certain metric $\|\cdot\|_{\cX}$ from the target function $f$. 
\section{General Approximation Theory}
\label{Sec: 4-Gen-App-The}
The most famous example in approximation theory is the least square problem 
$$\min_{x\in S} \|Ax - b\|_2$$
where $A\in\bbR^{N\times k}$ and a given vector $b\in \bbR^N$. Seeking for solution $x\in S = \bbR^k$ is the simplest case. The problem can be efficiently solved if $S$ is a convex set. 
\begin{definition}
    Let $\cM\subset \cV$ of a normed space $(\cV, \|\cdot\|)$ and given $v\in\cV$, the best approximation in $\|\cdot\|$ is 
    $$u^{\ast}\in\cM,\quad \|u^{\ast} - v\| = \inf_{u\in\cM} \|u - v\|$$
\end{definition}
\begin{definition}
    The sequence $u_k$, $k\in\bbN$ is an minimizing sequence if 
    \begin{equation}
        u_k \in\cM,\quad \|u_k - v\|\to \inf_{u\in\cM}\|u - v\|,\quad k\to\infty.
    \end{equation}
\end{definition}
\begin{theorem}[existence of best approximation]
\label{Thm: 4-Exi-Bes-App}
If $u_k$ is a minimizing sequence and has an accumulation point $u^{\ast}$ in $\cM$, then  $u^{\ast}$ is a best approximation to $v$.
\end{theorem}
\begin{proof}
    Just take the limit (subsequence) on both sides to 
    \begin{equation}
        \|u^{\ast} - v\| \le \|u^{\ast} - u_k\| + \|u_k - v\|.
    \end{equation}
\end{proof}
\begin{theorem}
\label{Thm: 4-Com-Exi}
    If $\cM$ is a compact subset of $\cV$, then the best approximation always exists.
\end{theorem}
\begin{proof}
    
\end{proof}
One special case is that $\cM$ is a finite dimension linear subspace of $\cV$, then one can take a bounded closed set truncating the minimizing sequence, then such set must be compact.
\begin{lemma}[convexity]
\label{Lem: 4-CON}
If $\cM$ is a convex set of normed space $\cV$, then the set of best approximations is convex.
\end{lemma}
\begin{proof}
    
\end{proof}
\begin{theorem}[uniqueness]
\label{Thm: 4-Uni-1}
    If $\cM$ is strictly convex of normed space $\cV$, then the best approximation is unique. It is worthwhile to notice that strictly convexity is sufficient but not necessary.
\end{theorem}
\begin{proof}
    
\end{proof}
\begin{definition}
   The normed space $(\cV, \|\cdot\|)$ is strictly normed if and only if the unit ball is strictly convex).
\end{definition}
\begin{theorem}[uniqueness]
\label{Thm: 4-Uni-2}
    If $\cM$ is a strictly normed linear subspace of normed space $\cV$, then there exists at most one best approximation for each $v\in\cV$.
\end{theorem}
\begin{proof}
    
\end{proof}

\begin{remark}
    The above theorems are quite preliminary in general Banach spaces. Formalizing the proofs with $\texttt{Lean}$ is recommended. See Exercises.  
\end{remark}
\subsection{Orthogonality in \texorpdfstring{$L^p$} - Spaces}
The orthogonality in $L^p$ spaces is defined similarly to $L^2$ spaces. Here $p\in(1,\infty)$.
\begin{definition}
    Let $H = L^p(\cX, \mu)$. If $\|f\|_p\le \|f + g\|_p$ for all $g\in H'\subset H$ in a closed subspace, then we say $f\perp H'$. 
\end{definition}
Since the $L^p$ norm is strictly convex, for any $f\in H$, there exists a unique $f^{\ast}\in H'\subset H$ as the best approximation in $H'$. It is not difficult to observe that $f-f^{\ast}\perp H'$. 

Using the Hahn-Banach theorem, we can prove the following lemma. 
\begin{lemma}
    Let $H'$ be a closed subspace of $H=L^p(\cX, \mu)$, $p\in(1,\infty)$. Then $f\perp H'$ is equivalent to 
    \begin{equation}
       \aver{ {|f|^{p-2}} \overline{f},  \overline{g} } = 0,\quad \forall g\in H'.
    \end{equation}
\end{lemma}
\begin{proof}
($\Leftarrow$) Assuming the equality, 
    \begin{equation}
      \|f\|_p^p = \aver{ |f|^{p-2} \overline{f}, \overline{f} } = \aver{|f|^{p-2} \overline{f},  \overline{f + g}} \le \| |f|^{p-2} \overline{f} \|_{p'} \|f+g\|_p = \|f\|_p^{p-1} \|f+g\|_p,
    \end{equation}
    which implies the orthogonality $\|f\|\le \|f + g\|$ for all $g\in H'$. 
    
($\Rightarrow$) Assuming the orthogonality, then by the Hahn-Banach theorem, there exists a linear functional $\cD\in L^{p,\ast}=L^{p'}$ that 
\begin{equation}
    \cD f = 1,\quad \cD H' = 0, \quad \|\cD\|= \|f\|_p^{-1}. 
\end{equation}
By Rietz representation theorem, we have an element $h\in L^{p'}$ that $\cD f = \aver{f, h}$ with $\|h\|_{p'}=\|f\|_p^{-1}$. Therefore, the H\"older inequality 
\begin{equation}
    1 =\cD f = \aver{f, h} \le \|f\|_p \|h\|_{p'} = 1. 
\end{equation}
The condition for equality is $f(x) h(x) \ge 0$ and $|h|^{p'} = \lambda |f|^p$ for certain $\lambda > 0$. Thus, $h \propto |f|^{p-2} \overline{f}$ and our conclusion is proved by $\cD H' = 0$.
\end{proof}
\section{Minimax Approximation}
\label{Sec: 4-MIN-APP}
Given $f\in C^0([a, b])$, find a polynomial $p_n\in\Pi_n$ such that $$\|f - p_n\|_{\infty} = \min_{g\in\Pi_n} \|f - g\|_{\infty}$$
Such a problem is a typical minimax approximation problem. In the previous Chapter~\ref{Ch: 2-Int}, we have seen a similar problem which is to minimize the maximum of $|\omega(x)|$ with $\omega(x) = \prod_{j=0}^n(x - x_j)$. The proof used there can be borrowed for the following theorem as well. 

\begin{theorem}[de la Vall\'ee-Poussin]
\label{Thm: 4-DE-LA-VAL-POU}
    Let $f\in C^0([a, b])$ and $n\ge 0$, let $x_0<x_1<\dots<x_{n+1}$ be $n+2$ nodes in $[a, b]$. If there exists a polynomial $q_n\in\Pi_n$ such that 
    $$f(x_j) - q_n(x_j) = (-1)^j e_j,\quad j =0,\dots, n+1.$$ 
    where $e_j$ are having the same sign and nonzero, then 
    \begin{equation}
        \min_j |e_j|\le E_n^{\ast}(f): = \min_{g\in\Pi_n} \|f - g\|_{\infty} \le \max_j |e_j|.
    \end{equation}
\end{theorem}

\begin{proof}
    Prove by contradiction. Suppose $E_n^{\ast}(f)$ is achieved by some polynomial $g\in\Pi_n$. If $q_n$ satisfies that 
    \begin{equation}
        |e_j| > E_n^{\ast}(f) = \|f - g\|_{\infty} \quad \forall j=0,1,\cdots n+1.
    \end{equation}
    Then, $q_n - g = q_n - f - (g - f)$ implies that 
    \begin{equation}
    \sgn(q_n - g)(x_j) = \sgn(q_n - f)(x_j) = -(-1)^j\sgn(e_j),
    \end{equation}
    which changes sign $n+1$ times while $q_n - g$ only has $n$ roots.
\end{proof}
The above theorem implies the sufficiency of the ``equioscillation'' property of length $n+2$ for the minimax approximation. The next theorem shows it is also necessary.
\begin{theorem}[Chebyshev equioscillation theorem]
\label{Thm: 4-Che-Equ}
    Let $f\in C^0([a, b])$ and $n\ge 0$. Then there exists a \emph{unique} polynomial $q^{\ast}\in\Pi_n$ such that 
    \begin{equation}
        E_{n}^{\ast}(f) = \|f - q_n^{\ast}\|_{\infty}.
    \end{equation}
    This polynomial is uniquely characterized by the property that 
    $\exists a\le x_0<\dots < x_{n+1}\le b$ 
    for which we can select $\sigma = \pm 1$ that 
    \begin{equation}\label{EQ: 4-EQUI-OSCI}
        f(x_j) - q_n^{\ast}(x_j) = \sigma (-1)^j \|f - q_n^{\ast}\|_{\infty} ,\quad j=0,1,\dots, n+1.
    \end{equation}
\end{theorem}
\begin{proof}
    The existence of such minimizing polynomial $q^{\ast}\in\Pi_n$ can be proved through a minimizing sequence argument. Let $q^k\in \Pi_n$ be a minimizing sequence to having $\|q^k - f\|\to E_n^{\ast}(f)$, then it is clear that the set 
    \begin{equation}
        \cM = \{q\in \Pi_n\mid \|q - f\|\le  E_n^{\ast}(f) + 1\}
    \end{equation}
    must be non-empty and such a set is compact since $\cM$ is of finite dimension and closed. Then the minimizing sequence will have a converging sub-sequence, so the limit sits in $\cM$.

    Then we show it is necessary (sufficiency by Theorem~\ref{Thm: 4-DE-LA-VAL-POU}) to have~\eqref{EQ: 4-EQUI-OSCI} for this minimizing polynomial $q_n$. If it is not satisfied, then we can partition the interval $[a, b]$ into $1\le N\le n+1$ parts 
    $$[a, b]=\cup_{j=1}^N [t_{j-1}, t_j],\quad a = t_0<\dots< t_N = b,$$
    such that 
    \begin{enumerate}
        \item $f(t_k) - q_n(t_k) = 0$, $k = 1,\dots, N-1$.
        \item for each $1\le k\le N$, there exists (may not be unique) $s_k\in [t_{k-1}, t_k]$ such that 
        $$|f(s_k) - q_n(s_k)| = \|f - q_n\|_{\infty}\neq 0$$
        and for any $x\in [t_{k-1}, t_k]$, 
        $$-(f(x) - q_n(x)) \neq (f(s_k) - q_n(s_k)).$$
        \item for each $1\le k\le N-1$, 
        $$f(s_k) - q_n(s_k) = -(f(s_{k+1}) - q_n(s_{k+1}) ).$$
    \end{enumerate}
    Without loss of generality, one can assume that 
    \begin{eqnarray}
        \sgn(f(s_k) - q_n(s_k)) = (-1)^{k-1}, 
    \end{eqnarray}
    then using the second condition, there exists $\eps$ such that $\forall x\in [t_{k-1}, t_k]$,
    \begin{equation}
        \begin{aligned}
            -\|f-q_n\|_{\infty} + \eps &\le  f(x) - q_n(x)\quad& k &\text{ is odd}\\
            f(x) - q_n(x) &\le \|f-q_n\|_{\infty} - \eps\quad& k &\text{ is even}     
        \end{aligned}
    \end{equation}
    Then we construct a polynomial $g\in \Pi_{N-1}$ that 
    \begin{eqnarray}
        \sgn(g(x)) = (-1)^k,\quad x\in[t_{k-1}, t_k].
    \end{eqnarray}
    and $\|g\|\le \frac{\eps}{2}$ and let $k(x) = q_n(x) - g(x)\in \Pi_{n}$ and $f(x) - k(x) = f(x)-q_n(x) + g(x)$, which implies that 
    \begin{enumerate}
        \item $f(x) - k(x) < f(x) - q_n(x)$, if $x\in (t_{k-1}, t_k)$ for $k$ odd.
        \item $f(x) - k(x)\ge - \|f - q_n\| + \frac{\eps}{2}$, if $x\in (t_{k-1}, t_k)$ for $k$ odd.
        \item $f(x) - k(x) > f(x) - q_n(x)$, if $x\in (t_{k-1}, t_k)$ for $k$ even.
        \item $f(x) - k(x)\le \|f - q_n\| -\frac{\eps}{2}$, if $x\in (t_{k-1}, t_k)$ for $k$ odd.
    \end{enumerate}
    In this way, $\|f - k\|_{\infty} < \|f - q_n\|_{\infty}$, which is a contradiction.

    In the last, we show the uniqueness. Suppose there are two polynomials $q_n, \tilde{q}_n$ being the minimax approximation. Then $\frac{1}{2}(q_n +\tilde{q}_n)$ must also be a minimax approximation, therefore there exist the alternation nodes 
    \begin{equation}
        a\le s_0 < s_1 <\dots < s_{n+1} \le b
    \end{equation}
    such that 
    \begin{equation}
        \frac{1}{2}(q_n - f)(s_k) + \frac{1}{2}(\tilde{q}_n - f)(s_k) = \sigma (-1)^k E_n^{\ast}(f)
    \end{equation}
    Therefore, we must have 
    \begin{equation}
        (q_n - f)(s_k) = (\tilde{q}_n - f)(s_k)\Rightarrow q_n(s_k) = \tilde{q}_n(s_k),
    \end{equation}
    which implies $q_n = \tilde{q}_n$ by the uniqueness of interpolation. 
\end{proof}

\begin{remark}
    If the approximation is under $L^{2}$ norm instead of $C^0$ norm, then  
    \begin{equation}
        f_n^{\ast} = \argmin_{f_n \in \Pi_n }\|f - f_n\|_{L^2[a, b]} = \sum_{k=0}^{n} a_k p_k
    \end{equation}
    where $\{p_k\}_{k\ge 0}$ is the set of orthogonal polynomials (Legendre polynomials) on $[a, b]$ and the coefficients $a_k = \aver{f, p_k}/\aver{p_k, p_k}$. 
\end{remark}

\subsection{Remez Algorithm}
The Chebyshev equioscillation theorem does not provide a constructive way to find the best polynomial approximation in $L^{\infty}$ norm. However, the numerical method to find the minimax polynomial could adopt the idea of the above Chebyshev equioscillation theorem. Intuitively, the aiming polynomial will be oscillatory compared with $f$, therefore we can start with the Chebyshev polynomial approximation, which is
\begin{equation}
    C_n(x) = \sum_{j=0}^n c_j T_j(x),\quad c_j = \aver{f, T_j}_w/\aver{T_j,T_j}_w
\end{equation}
where $w = (1-x^2)^{-1/2}$. The convergence is uniform as $n\to \infty$, especially if $f\in C^r([a, b])$, the coefficient $c_j\sim \cO(j^{-r})$ decays fast, then the reminder approximately $f - C_n\simeq c_{n+1}T_{n+1}$, this reminder achieves equioscillation at exactly $(n+2)$ points. The Remez algorithm (see Algorithm~\ref{ALG: REMEZ}) is based on the above idea and performs an iterative construction.
\begin{algorithm}[!htbp]
    \SetAlgoLined
    \caption{Remez algorithm for polynomial approximation on $[-1,1]$}    \label{ALG: REMEZ}
    \KwData{Objective function $f(x)$ on $[-1, 1]\subset \mathbb{R}$}
    \KwResult{Polynomial $p(x)\in\Pi_n$ and estimated error $E$}
    $\{x_j\}_{j=0}^n\gets \texttt{findExtreme}(T_{n+1})$//  Initialize local extrema nodes of $T_{n+1}$; 
    \\
    \While{True}{
        Solve the equation for $a_j$ and $E$,
        \begin{equation}
            \sum_{j=0}^{n} a_j x_k^j + (-1)^k E = f(x_k),\quad k = 0, \cdots n+1;
        \end{equation}\\
        $p(x) \gets \sum_{j=0}^{n} a_j x^j$; // new candidate polynomial; \\ 
        $\text{stop} \gets \texttt{isEquioscillation}(p - f)$ // check equioscillation condition~\eqref{EQ: 4-EQUI-OSCI} ; \\
        \eIf {stop}{ 
            return \\
        }
        {
            $\{x_j\}_{j=0}^n\gets \texttt{findExtreme}((p - f))$ // find local extrema nodes of $p - f$.
        }
    }
\end{algorithm}
\subsection{Polynomial Approximation for Analytic Functions}
Let $f(z)$ be an analytic function inside a domain
$D_{\rho}\subset \bbC$, where a finite interval $I\subset D_{\rho}$. Usually, $f$ can be well approximated by polynomials in $D_{\rho}$. Here, the parameter $\rho$ and the definition of $D_{\rho}$ will be elaborated later. In particular,   
\begin{equation}
    E_{n,\infty}(f) = \min_{f_n\in \Pi_n} \|f - f_n\|_{L^{\infty}(I)}
\end{equation}
decays very quickly in terms of $n$. Let $p_n\in \Pi_n$, $n = 0,1,\cdots$ be the orthogonal polynomials over $I\subset \bbR$ with the analytic weight $w$.  Suppose one can represent (pointwise convergence)
\begin{equation}
\label{EQ: 4-COEF-POLY}
    f(x) = \sum_{n=0}^{\infty} a_n p_n(x), \quad a_n = \frac{\aver{f, p_n}_w}{\aver{p_n, p_n}_w}, 
\end{equation}
then, $E_{n, \infty}(f) \le  \sum_{k > n} | a_k | \sup_{I} \left| p_k \right|$. Therefore, we only have to estimate the coefficients $|a_n|$. 
\begin{definition}[weighted Cauchy transform]
\label{Def: 4-Wei-Cau-Tra}
Let $x\in \bbR$ and suppose $w(x) p_n(x)$ satisfies that 
\begin{equation}
    |w(x) p_n(x)| \le C_n \max_{1\le j\le J} \frac{1}{|x - t_j|^{\alpha}}, \quad \alpha \in (0, 1)
\end{equation}
for a finite number of $t_j\in \bbR$. Then the weighted Cauchy transform of $p_n$ is defined by 
\begin{equation}
    P_n(z) = \frac{1}{2\pi i} \int_{I} \frac{w(x) p_n(x)}{z - x} dx,\quad z \in \bbC - \overline{I}. 
\end{equation}
\end{definition}
Clearly, $P_n(z)$ is a holomorphic function in $\bbC - \overline{I}$. In addition, if $|z|\to \infty$, then $|P_n(z)| = \cO(|z|^{-1})$. 
\begin{lemma}[Plemelj formula]
\label{Lem: 4-PLE-For}
    For any $x\in I$, then 
    $$\lim_{\eps\to 0^{+}}P_n(x - \eps i) - P_n(x + \eps i) = w(x) p_n(x). $$ 
\end{lemma}
\begin{proof}
    Let $\delta_{\eps}(x) =  \frac{1}{\pi} \frac{\eps}{x^2 + \eps^2}$ and notice that 
    \begin{equation}
    \begin{aligned}
           P_n(x - \eps i) - P_n(x + \eps i) &= \frac{1}{2\pi i } \int_I w(y)p_n(y) \left(\frac{1}{x - \eps i - y}  - \frac{1}{x + \eps i - y} \right) \\ 
           &= \int_I \delta_{\eps}(x-y) \ast  w(y) p_n(y)  dy \\
           &\to  w(x) p_n(x).
    \end{aligned}
    \end{equation}
\end{proof}
\begin{lemma}
\label{Lem: 4-Coe-Pol}
    If $f$ is analytic and bounded in the domain $D_{\rho}$ and the weight $w$ satisfies the condition in Definition~\ref{Def: 4-Wei-Cau-Tra}, 
    then the projection can be computed by the contour integral
    \begin{equation}
        \aver{f, p_n}_w = \int_{\cC_{\rho}} f(z) P_n(z) dz,
    \end{equation}
    where the contour $\cC_{\rho} = \partial D_{\rho}$.
\end{lemma}
\begin{proof}
    Since the functions $f(z)$ and $p_n(z) w(z)$ are analytic in $D_{\rho}$ except a finite number of points. Due to the mild growth of $w(x)p_n(x)$, the infinitesimal contours around these points will not contribute. Therefore, the contour $\cC_{\rho}$ can be deformed in the lower and upper half-planes to the $x$ axis. Then, use Lemma~\ref{Lem: 4-PLE-For}, 
    \begin{equation}
         \int_{\cC} f(z) P_n(z) dz = \lim_{\eps\to 0} \int_{I}  P_n(x - \eps i) + P_n(x + \eps i) f(x) dx = \int_{I} f(x) p_n(x) w(x) dx. 
    \end{equation}
    The Lemma~\ref{Lem: 4-Coe-Pol} implies a trivial bound by the maximal modulus principle.
\end{proof}
The following examples estimate the approximation error for Chebyshev polynomials and Legendre polynomials. The asymptotic expressions of the Cauchy transform are from~\cite{elliott1974asymptotic} for large $n$.  
\begin{example}
    For Chebyshev polynomials on $I = [-1,1]$ that $w(x) = (1 - x^2)^{-1/2}$, then $|p_n|$ are bounded by one,  
    \begin{equation}
        P_n(z) \sim \frac{1}{2i (z^2 - 1)^{1/2} [z + (z^2 - 1)^{1/2}]^n},\quad z \notin [-1, 1].
    \end{equation}
    Therefore, if we choose $D_{\rho} = \{|z + (z^2 - 1)^{1/2}| \le \rho\}$ for $\rho > 1$, the domain is an ellipse with foci at $\pm 1$, the ellipse is also called the Berstein ellipse. Then use $\aver{p_n, p_n} = \frac{\pi}{2}$ for $n > 1$, 
    \begin{equation}
        |a_n| \sim \frac{\sup_{\cC_{\rho}} |f|}{\pi \rho^n } \int_{\cC_{\rho}} \frac{1}{|(z^2 - 1)|^{1/2}} d|z| = \frac{2}{\rho^n} \sup_{\cC_{\rho}} |f|.
    \end{equation}
    This implies the approximation error $E_{n,\infty}(f) \le \sum_{k > n} \frac{2}{\rho^k} \sup_{\cC_{\rho}} |f| = \cO(\rho^{-n})$.
\end{example}
\begin{example}
    For Legendre polynomials on $I = [-1, 1]$ that $w(x) = 1$ and $\aver{p_n, p_n} = \frac{2}{2n+1}$, the polynomials $|p_n|$ are bounded by one, then 
    \begin{equation}
        P_n(z) \sim \frac{2^{2n+1/2}|\Gamma(n+1)|^2}{i\pi \Gamma(2n+2)(z+(z^2-1)^{1/2})^{n+1/2}} (z^2-1)^{-1/4},\quad z\notin [-1,1].
    \end{equation}
    Then in the Berstein ellipse $D_{\rho}$, 
    \begin{equation}
        |a_n|\sim \frac{\sup_{\cC_{\rho}}|f|}{\pi \rho^{n + 1/2} } \frac{2^{2n-1/2}(n!)^2 }{ (2n)!  }\int_{\cC_{\rho}} \left|(z^2 - 1)^{-1/4}\right| d|z| = \cO\left(\frac{\sqrt{ n}\sup_{\cC_{\rho}|f|}}{\rho^n}\right).
    \end{equation}
    Therefore, if $n > |\log \rho|^{-1}$ is sufficiently large, the approximation error satisfies
    \begin{equation}
    \begin{aligned}
        E_{n,\infty}(f) &= \cO\left(\sup_{\cC_{\rho} } |f| \sum_{k>n}\frac{\sqrt{k}}{\rho^k}\right) = \cO(\sqrt{n}\rho^{-n}).
    \end{aligned}
    \end{equation}
\end{example}
\begin{remark}
    It can be shown for the general Jacobi polynomials $p_n$ with weight $(1-x)^{\alpha}(1+x)^{\beta}$, the coefficient 
    $|a_n| = \cO(\sqrt{n} \rho^{-n} )$ as the previous example. However, if $q = \max(\alpha, \beta)$, then 
    \begin{equation}
        E_{n,\infty}(f) = \sum_{k>n} \cO\left(\sqrt{k} \rho^{-k} \sup_{[-1,1]} |p_k|\right) = \begin{cases}
            \cO\left( n^{q+\frac{1}{2}}\rho^{-n}\right),\quad & q\ge -\frac{1}{2},        \\
             \cO\left( \rho^{-n} \right),\quad & q < -\frac{1}{2}. 
            \end{cases}
    \end{equation}
    In other words, the Chebyshev polynomials have attained the best bound. The approximation error by Legendre polynomials only differs by a factor of $\sqrt{n}$. 
\end{remark}
\subsection{Polynomial Approximation for Differentiable Functions}
If $f\in C^{m-1}(I)$ with $f^{(m)}\in L^{\infty}(I)$ instead of being analytic, it is intuitive to conjecture that the coefficients $|a_n| = \cO(n^{-m})$ are an analog to the Fourier approximation theory. Unlike the analytic case, extending the definition of $f$ to the Berstein ellipse is not applicable. 

Let us focus on the classical orthogonal polynomials which obey Rodrigues' formula (see Theorem~\ref{Thm: 3-Rod-For}). 
\begin{equation}
    p_n(x) w(x) = c_n \frac{d^n}{dx^n} \left[w(x) |g(x)|^n\right],  
\end{equation}
where $\frac{w'}{w} = \frac{h}{g}$ for $h\in \Pi_1$ and $g\in \Pi_2$. 
\begin{enumerate}
    \item Jacobi polynomials: $w(x) = (1-x)^{\alpha} (1+x)^{\beta}$ where $\alpha, \beta>-1$, and $\frac{w'}{w} = \frac{-\alpha(1+x) + \beta(1-x)}{1-x^2}$. 
    \item Laguerre polynomials: $w(x) = x^{\alpha} e^{-x}$ where $\alpha>-1$ and $\frac{w'}{w} = \frac{\alpha-x}{x}$. 
    \item Hermite polynomials: $w(x) = e^{-x^2}$ and $\frac{w'}{w} = -2x$. 
\end{enumerate}
Denote $Q(x) = c_n w(x) |g(x)|^n$. Since $w(x)g(x)|_{\partial I} = 0$, $\frac{d^k}{dx^k} (w(x) |g(x)|^n )|_{\partial I} = 0$ for $k <n$.  We apply integration by parts to~\eqref{EQ: 4-COEF-POLY} $m$ times ($m < n$). 
\begin{equation}
\begin{aligned}
    a_n &= \frac{1}{ \aver{p_n, p_n}_w}\int_I f(x) p_n(x) w(x) dx \\
    &= \frac{1}{ \aver{p_n, p_n}_w}\left( f'(x) Q^{(n-1)}(x)\Big|_{\partial I} - \int_{I} f'(x) Q^{(n-1)}(x) dx \right) \\
    &= \cdots \\
    &= \frac{(-1)^m }{\aver{p_n, p_n}_w} \int_{I} f^{(m)}(x) Q^{(n-m)}(x) dx. 
\end{aligned}
\end{equation}
Therefore, we only have to estimate $\|Q^{(n - m)}\|_{\infty}$. Using Rodrigues' formula for Jacobi polynomials, it is straightforward to derive the following identity.
\begin{lemma}
\label{Lem: 4-Int-Ort-Pol}
    For Jacobi polynomials $p_n = P_{n}^{\alpha, \beta}$,  the constant $c_n = \frac{(-1)^{n}}{2^n n!}$ and \begin{equation}
        Q^{(n - m)}(x) = \frac{c_n}{c_{n-m}} (1-x)^{\alpha+m}(1+x)^{\beta+m} P_{n-m}^{a+m, \beta + m}(x)
    \end{equation}
\end{lemma}
\begin{proof}
    Notice that 
    \begin{equation}\nonumber
    \begin{aligned}
        c_n \frac{d^{n-m}}{d x^{n-m}} \left[(1-x)^{\alpha}(1+x)^{\beta} |(1-x^2)|^n\right] &= c_n  \frac{d^{n-m}}{d x^{n-m}}  \left[ (1-x)^{m+\alpha}(1+x)^{m+\beta} |(1-x^2)|^{n-m} \right] \\
        &=  \frac{c_n}{c_{n-m}} p_{n-m}^{\alpha+m, \beta+m}(x) (1 - x)^{\alpha+m} (1+x)^{\beta+m}. 
    \end{aligned}
    \end{equation}
\end{proof}
The following estimate is from~\cite{nevai1994generalized} for $\alpha, \beta > -\frac{1}{2}$, which has been improved in a later paper~\cite{krasikov2005maximum}. 
\begin{theorem}
\label{Thm: 4-Ort-Poly-Der-Est}
    $\max_{[-1,1]} (1-x)^{\alpha+\frac{1}{2}}(1+x)^{\beta+\frac{1}{2}} \left|P^{\alpha,\beta}_n(x)\right|^2 \le \frac{2e (2 + \sqrt{\alpha^2+\beta^2})}{\pi}$. 
\end{theorem}
Note that the above bound is independent of $n$. Therefore, when $m \ge 1$, there exists a constant $C(\alpha, \beta, m)$ such that 
\begin{equation}
    |Q^{(n-m)}(x)| \le  C(\alpha, \beta, m)\left| \frac{c_n}{\gamma_n c_{n-m}} \right|.
\end{equation}
Since $Q^{(n-m)}$ is bounded by the product of the following terms.
\begin{equation*}
\begin{aligned}
    &\sup_{[-1,1]} (1-x)^{\frac{\alpha+m}{2}-\frac{1}{4}} (1+x)^{\frac{\beta+m}{2} - \frac{1}{4}} \le 2^{\frac{|\beta-\alpha|}{2}} \\
    &\sup_{[-1, 1]} \left| p_{n-m}^{\alpha+m, \beta+m}(x) (1 - x)^{\frac{\alpha+m}{2}+\frac{1}{4}} (1+x)^{\frac{\beta+m}{2}+\frac{1}{4}}  \right|\le \left|\frac{2e(2 + \sqrt{ (\alpha+m)^2 + (\beta+m)^2 })}{\pi}\right|^{1/2}.
\end{aligned}
\end{equation*}
% \begin{theorem}
% \label{Thm: 4-Jac-Bou-Coe}
% For Jacobi polynomials $p_n = P_{n}^{\alpha, \beta}$, the coefficient $a_n$ satisfies 
%    $$|a_n|\le \frac{2^{|\beta-\alpha|/2+1}}{\gamma_n} \frac{\sqrt{2e(2+\sqrt{2}(q+m))/\pi}}{2^m n(n-1)\cdots (n - m+1)}\|f^{(m)}\|_{L^{\infty}}, \quad q= \max(\alpha, \beta), $$
% where 
%     \begin{equation}
%         \gamma_n = \aver{p_n, p_n} = \frac{2^{\alpha + \beta + 1}}{2n+\alpha + \beta + 1} \frac{\Gamma(n+\alpha+1)\Gamma(n+\beta + 1)}{\Gamma(n+\alpha+\beta + 1)\Gamma(n+1)}.
%     \end{equation}
% \end{theorem}
% \begin{example}
% For $\alpha = \beta = 0$, the coefficient for Legendre polynomial expansion satisfies $$|a_n|\le \frac{(2n+1) \sqrt{2e(2+\sqrt{2}m)/\pi} }{2^{m} n(n-1)\cdots (n - m+1)}\|f^{(m)}\|_{L^{\infty}}. $$
% \end{example}
% \begin{example}
% For $\alpha = \beta = -\sfrac{1}{2}$, note the Chebyshev polynomials $T_n\sim \cO(\sqrt{n}) P_n^{-\sfrac{1}{2}, -\sfrac{1}{2}}(x)$, the coefficient for Chebyshev polynomial expansion satisfies 
% $$ |a_n| \le \frac{2^{2n+1}}{\pi\binom{2n}{n}}\frac{\sqrt{2e(2+\sqrt{2}(m-\frac{1}{2}))/\pi}}{2^m n (n-1)\cdots (n-m+1)} \|f^{(m)}\|_{L^{\infty}}. $$
% Note that $\frac{2^{2n+1}}{\pi\binom{2n}{n}}\sim 2\sqrt{\frac{n}{\pi}}$ by Stirling formula. This bound is worse than the estimate in~\cite{trefethen2019approximation} by a factor if $\sqrt{n}$ due to the loose bound from Theorem~\ref{Thm: 4-Ort-Poly-Der-Est}. 
% \end{example}

\begin{theorem}
    For Jacobi polynomials $p_n = P_n^{\alpha, \beta}$, the coefficient $a_n$ satisfies 
    \begin{equation}
        |a_n| \le \frac{C'(\alpha, \beta)\|f^{(m)}\|_{\infty}}{\gamma_n}  \frac{1}{n(n-1)\cdots (n-m)} \frac{1}{\sqrt{n-m}}
    \end{equation}
    for $n$ sufficiently large.
\end{theorem}
\begin{proof}
By replace $x = \cos\theta$, then 
\begin{equation}\nonumber
    P_{n-m}^{m+\alpha, m+\beta}(x) (1-x)^{m+\alpha}(1+x)^{m+\beta} = P_{n-m}^{m+\alpha, m+\beta}(\cos\theta) \left(2\sin^2 \frac{\theta}{2}\right)^{m+\alpha}\left(2\cos^2\frac{\theta}{2}\right)^{m+\beta}.
\end{equation}
Using the Darboux formula for large $n$ and $\theta\in [c (n-m)^{-1}, \pi - c(n-m)^{-1}]$, see Theorem 8.21.13 in~\cite{szeg1939orthogonal}, we have the following asymptotic expression.  
\begin{equation}\nonumber
\begin{aligned}
&P_{n-m}^{m+\alpha, m+\beta}(\cos\theta) (2\sin^2 \frac{\theta}{2})^{m+\alpha}(2\cos^2\frac{\theta}{2})^{m+\beta}\\
&=    2^{2m+\alpha+\beta}|n-m|^{-\frac{1}{2}}\left( \pi^{-\frac{1}{2}}\sin^{m+\alpha-\frac{1}{2}}\frac{\theta}{2} \cos^{m+\beta-\frac{1}{2}}\frac{\theta}{2} \cos(N\theta+\gamma) + \cO((|n-m|\sin\theta)^{-1})\right), 
\end{aligned}
\end{equation}
where $N = n + \frac{\alpha+\beta+1}{2}$ and $\gamma = -\frac{\pi}{2}(m + \alpha + \frac{1}{2})$. 
    Because the coefficient 
    \begin{equation}
    \begin{aligned}
        |a_n| &\le \frac{1}{\gamma_n} \int_{I} |f^{(m)}(x) Q^{(n-m)}(x) | dx  \\
        &\le \frac{1}{\gamma_n} \int_{I_c} |f^{(m)}(x) Q^{(n-m)}(x) | dx + \frac{1}{\gamma_n} \int_{I_c'} |f^{(m)}(x)Q^{(n-m)}(x)| dx 
    \end{aligned}
    \end{equation}
    where $I_c = \{\theta\mid c (n-m)^{-1}\le \theta\le \pi - c(n-m)^{-1} \} = \{x\in I\mid -1+\frac{c'}{|n-m|^2} \le x \le 1 - \frac{c'}{|n-m|^2}\}$ and $I_c' = I - I_c$. We also have
    \begin{equation}
    \label{EQ: 4-BOU-JAC}
        2^{m+\frac{\alpha}{2} + \frac{\beta}{2} - \frac{1}{2}} \sin^{m+\alpha-\frac{1}{2}}\frac{\theta}{2} \cos^{m+\beta-\frac{1}{2}}\frac{\theta}{2}  = (1-x)^{\frac{1}{2}(m+\alpha-\frac{1}{2})} (1+x)^{\frac{1}{2}(m+\beta-\frac{1}{2})} \le 2^{|\beta-\alpha|/2}.
    \end{equation}
    Therefore, applying Theorem~\ref{Thm: 4-Ort-Poly-Der-Est} and neglecting the smaller term, there exists a constant $C'$ such that
    \begin{equation}
    \begin{aligned}
    |a_n| &\le \frac{1}{\gamma_n}  \left( |I_c|\|f^{(m)}\|_{L^{\infty}(I_c)} \sup_{I_c} |Q^{(n-m)}| + |I_c'|\|f^{(m)}\|_{L^{\infty}(I_c')} \sup_{I_c'} |Q^{(n-m)}(x)| \right) \\
    &\le\left| \frac{c_n}{\gamma_n c_{n-m}} \right|\|f^{(m)}\|_{\infty} \left[  2^{m+q+\frac{1}{2}}\cO(|n-m|^{-1/2})   + \cancel{C(\alpha,\beta, m)\cO(|n-m|^{-2})}\right]\\
    &\le \frac{C'(\alpha, \beta)}{\gamma_n} \frac{1}{n(n-1)\cdots (n-m)}|n-m|^{-\frac{1}{2}} \|f^{(m)}\|_{\infty}
    \end{aligned}
    \end{equation}
    for sufficiently large $n$. 
\end{proof}
\begin{remark}
    Now it recovers the optimal rate $\cO(n^{-m})$ (resp. $\cO(n^{-m+\frac{1}{2}})$) for Chebyshev polynomials (resp. Legendre) when $n$ is sufficiently large. The constant in $\cO$ depends only on $\|f^{(m)}\|_{\infty}$ and $\alpha, \beta$.
\end{remark}

\section{Pad\'e Approximation}
Instead of linearly combining the basis functions for approximation, it is also possible to introduce nonlinear dependencies of the parameters for approximation. The most common one is the rational approximation. It uses the rational functions 
\begin{equation}
    R(x) = \frac{a_0 + a_1 x + \cdots + a_n x^n}{1 + b_1 x + \cdots + b_m x^m}
\end{equation}
to approximate the objective function $f$, which permits an expansion $f(x) = \sum_{k=0}^{\infty} c_k x^k$. The Pad\'e approximation solves the equations 
\begin{equation}
    R^{(q)}(0) = f^{(q)}(0),\quad q = 0, 1, \cdots, m+n.
\end{equation}
\begin{lemma}[existence and uniqueness]
    
\end{lemma}
% \section{Rank one approximation}
\section{Neural Network}
The neural network (NN) is a popular model for constructing nonlinear function approximations. Next, we introduce the fundamentals and leave the rest to later topics. The basic structure of NN consists of several layers of neurons that are connected through linear maps and activation functions. 

\begin{example}[Feed-forward]
Let $\bff_0(\bx) = \bx\in \bbR^{n_0}$ and $W_k\in \bbR^{n_{k-1}\times n_k}$, $\bb_k \in \bbR^{n_k}$, 
\begin{equation}
    \bff_k(\bx) = \sigma(W_k \bff_{k-1}(\bx) + \bb_k) , \quad k=1, 2,\cdots, K.
\end{equation}
and
$\bff_{K+1} = W_{k+1} \bff_k(\bx)$. 
The activation function $\sigma$ is a nonlinear function. 
\end{example}
\subsection{Universal Approximation Theorem}
The universal approximation theorem validates the feasibility of applying neural networks for approximation when the activation function is not polynomial. 
\subsection{Shallow Neural Network}
The shallow neural networks (with $K=1$ hidden layer) can be simplified to 
\begin{equation}
    \bff(\bx) = W_{2}\sigma(W_1 \bx + \bb_1).
\end{equation}
For simplicity, we assume that the function $\bff: I\mapsto \bbR$ for some compact set $I\subset \bbR$. The neural network can be formulated in an explicit form. 
\begin{equation}
    \bff(\bx) = \sum_{l=1}^n a_l \sigma(w_l \bx - b_l),\quad a_l, w_l, b_l\in\bbR.
\end{equation}
\begin{example}
    If the parameters $w_l\equiv 1$, then the function $\bff$ is a linear combination of shifted spline basis $\sigma(\bx -\cdot)$, 
    \begin{equation}
     \bff(\bx) = \sum_{l=1}^n a_l \sigma(\bx - b_l).
    \end{equation}
    For example, if $\sigma(x - \cdot)$ is a spline function, then the network is a spline approximation with trainable shifts.
\end{example}

\subsection{Gradient-Flow}
The training stage involves an optimization process that minimizes an empirical loss function.
\section{Exercises}
The source code and tests for the computational part are hosted on GitHub.
\subsection{Theoretical Part}
\begin{problem}
    Show that the vector space $C^0([a, b])$ with $\|\cdot\|_{\infty}$ is not strictly normed.
\end{problem}
\begin{problem}
    Let $f(x)\in C^0([-1,1])$ is even (odd). Show that the minimax approximation $q_n(x)\in\Pi_n$ is also even (odd).
    Hint: Think about the function $\frac{1}{2}(q_n(x) \pm q_n(-x))$, then use the Chebyshev equioscillation theorem and the uniqueness.
\end{problem}
\begin{problem}
    Let $\cS_k = \{x^{\lambda_1}, x^{\lambda_2}, \cdots, x^{\lambda_k}\}$ that $0\le \lambda_1 < \lambda_2 < \cdots < \lambda_k$, show that 
    \begin{equation}\nonumber
        \min_{f\in \textrm{span}(\cS_k)} \|f - x^{n}\|_{L^2[0, 1]} = \frac{1}{\sqrt{2n+1}}\prod_{j=1}^k\left|\frac{n - \lambda_j}{n + \lambda_j + 1}\right|.
    \end{equation}
    Show that the span of set $\lim_{k\to\infty}\cS_k$ is dense in $L^2[0, 1]$ if 
    \begin{equation}
      \lim_{k\to\infty}  \sum_{\lambda\in \cS_k} \frac{1}{\lambda+\sfrac{1}{2}} = \infty. 
    \end{equation}
\end{problem}
\begin{problem}
    Prove the Jacobi polynomials $P_n^{\alpha, \beta}(x)$ achieve maximum/minimum at $x= \pm 1$ if $\alpha, \beta \ge -\frac{1}{2}$.  
\end{problem}

\begin{problem}
    TODO: analyze gradient flow.
\end{problem}
\subsection{Computational Part}
\begin{problem}
    Implement the Remez algorithm and find the minimax approximation in $\Pi_3$ for the function $f(x) = e^x$ on $[0, 1]$.
\end{problem}

\begin{problem}
    Formalize the theorem/proofs in Section~\ref{Sec: 4-Gen-App-The} with \texttt{Lean}.
\end{problem}

\nocite{trefethen2019approximation,cheney2009course, muskhelishvili2008singular, elliott1974asymptotic}
\bibliographystyle{apalike}
\bibliography{chap4}
