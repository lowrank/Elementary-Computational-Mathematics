
\chapter{Floating Point Arithmetic}
\label{Ch: 1-Flo-Poi-Ari}

In this chapter, we will introduce some basics of the real number system for modern computers and discuss the arithmetic operations of the number system.

\section{Representation of Real Numbers}
\label{Sec: 1-Rep-Rea-Num}

Any \emph{nonzero} real number $x\in \bbR$ can be accurately represented with an infinite sequence of digits. This can be understood as the consequence that rational numbers are dense on any interval. Therefore, with the binary representation, we can write 
$$x = \pm (0.d_1 d_2 d_3\dots d_{t-1} d_t d_{t+1}\dots) \times 2^e,$$
where $e$ is an integer exponent and $d_1=1$, the other binary digits $d_i\in \{0, 1\}$. The mantissa part
$$0.d_1 d_2 d_3\dots =  \frac{d_1}{2} + \frac{d_2}{2^2} + \frac{d_3}{2^3} + \cdots.$$

\begin{remark}
    To guarantee the uniqueness of the above representation, we need a further assumption that there exists an infinite subset $S\subset \bbN$ that $d_j\neq 1$ for $j\in S$. For example, under binary representation
    $$0.111\dots = (0.1)\times 2^1,$$
    then we will take the latter representation.
\end{remark}

\section{Floating Point Numbers}
\label{Sec: 1-Flo-Poi-Num}

The floating point numbers generally refer to a set of real numbers with \emph{finite} mantissa length. More precisely, we consider the set of real numbers $\mathbb{F} = \mathbb{F}(t, e_{\min}, e_{\max})\subset \bbR$ that
$$\mathbb{F} := \{x\in \bbR\mid x = \pm (0.d_1 d_2 d_3\dots d_{t-1} d_t) \times 2^e, d_1 =1,  e_{\min}\le e\le e_{\max}\}\cup \{0\}.$$
It can be seen that there are only finite numbers in $\mathbb{F}$ with the smallest positive element $x_{\min} = 2^{e_{\min}-1}$ and the largest element $x_{\max} = ( 1- 2^{-t} )\times 2^{e_{\max} }$. Therefore $$\mathbb{F}\subset \overline{\mathbb{F}}:= \{ x\in \bbR\mid x_{\min} \le |x| \le x_{\max}\}\cup \{ 0\}.$$

\begin{remark}
    The elements in $\mathbb{F}$ are called normalized. If we allow $d_1 = 0$ in the definition of $\mathbb{F}$, then the numbers in the set are called denormalized.
\end{remark}

\begin{theorem}[Distribution of floating numbers]
\label{Thm: 1-Dis-flo-num}
    For any $e_{\min} \le e\le e_{\max}$, the distribution of the floating point number system $\mathbb{F}$ on interval $[2^{e-1}, 2^e]$ is equidistant with distances of length $2^{e-t}$.
\end{theorem}

\begin{proof}
    For any  $x \in \mathbb{F}\cap [2^{e-1}, 2^e]$ it can be represented by 
    $$x = (0.d_1d_2\dots d_t)\times 2^{e}$$
    where $d_1 = 1$. The mantissa is equidistantly distributed with distance $2^{-t}$, therefore the floating point numbers are equidistantly distributed with distances of length $2^{e-t}$. 
\end{proof}

To understand the approximation to real numbers by the floating point number system $\mathbb{F}$, it is important to consider the maximal relative distance between the numbers in $\overline{\mathbb{F}}$ and their respective closest element in $\mathbb{F}$, which is the following quantity:
$$\max_{x\in \overline{\mathbb{F}}, x\neq 0}\min_{z\in\mathbb{F}} \frac{|z - x|}{|x|}.$$
The following holds:
\begin{theorem}[Machine precision]
\label{Thm: 1-Mac-pre}
    $$\max_{x\in \overline{\mathbb{F}}, x\neq 0}\min_{z\in\mathbb{F}} \frac{|z - x|}{|x|}\le  2^{-t}.$$
    The number $\mathrm{u}:= 2^{-t}$ is also called rounding unit or machine precision.
\end{theorem}
\begin{proof}
    Without loss of generality, we only need to consider the positive numbers in $\overline{\mathbb{F}}$, then one can represent any nonzero $x\in [x_{\min}, x_{\max}]$ by
    $$x = (0.d_1 d_2 \dots d_t\dots)\times 2^e\in [2^{e-1}, 2^e].$$
    Since the floating point numbers are equidistantly distributed on $[2^{e-1}, 2^e]$ from Theorem~\ref{Thm: 1-Dis-flo-num}, one can find $z^{\ast}\in\mathbb{F}$ such that $$|z^{\ast} - x| \le \frac{1}{2} 2^{e-t},$$
    therefore 
    $$\frac{|z^{\ast}- x|}{|x|}\le \frac{1}{2} 2^{e-t} \frac{1}{2^{e-1}} = 2^{-t}.$$
\end{proof}

\begin{remark}
On modern computers, the following two floating point number systems $$\mathbb{F}_{32}:= \mathbb{F}(24, -125, 128),\quad\mathbb{F}_{64}:= \mathbb{F}(53, -1021, 1024)$$ are supported, they are often called single precision and double precision, respectively.  
\end{remark}

\section{Rounding}
\label{Sec: 1-Rou}
The rounding operation $\textsc{fl}$ is to map any real numbers of $\overline{\mathbb{F}}$ into the floating point number system $\mathbb{F}$ with the smallest error. Such rounding operation can be written out explicitly, let $ = \pm (0.d_1 d_2\dots d_t d_{t+1}\dots )\times 2^e$, then

\begin{equation*}
    \textsc{fl}(x) =\begin{cases}
        \pm (0.d_1 d_2\dots d_t) \times 2^e & \text{if } d_{t+1} = 0,\\
        \pm (0.d_1 d_2\dots d_t  + 2^{-t}) \times 2^e & \text{if } d_{t+1} = 1.
    \end{cases}
\end{equation*}
It is clear that rounding $\textsc{fl}$ is monotone and idempotent, which means
\begin{itemize}
    \item $x\le y \Rightarrow \textsc{fl}(x) \le \textsc{fl}(y)$. 
    \item $\textsc{fl}(z) = z$ if $z\in \mathbb{F}$.
\end{itemize}


\begin{theorem}[Rounding error]
\label{Thm: 1-Rou-err}
    For any $x\in \overline{\mathbb{F}}$, $|\textsc{fl}(x) - x| = \min_{z\in \mathbb{F}} |z - x|$. If $x\neq 0$, then
    $$\frac{|\textsc{fl}(x) - x|}{|x|}\le \mathrm{u} = 2^{-t}.$$
\end{theorem}

\begin{proof}
    The special case that $x = 0$ is trivial, we only consider $x\in [x_{\min}, x_{\max}]$, it can be seen that 
    \begin{equation*}
        |\textsc{fl}(x) - x| = | (0.d_1d_2\dots \tilde{d}_t) - (0.d_1d_2\dots d_t d_{t+1} \dots )|\times 2^e \le 2^{-(t+1)} \times 2^e,
    \end{equation*}
    where $\tilde{d}_t$ is the rounding bit, therefore 
    \begin{equation*}
        \frac{|\textsc{fl}(x) - x|}{|x|} \le \frac{2^{e-(t+1)}}{2^{e-1}} = 2^{-t}.
    \end{equation*}
\end{proof}

\begin{corollary}
\label{Cor: 1-Rel-err}
    For any $x\in\overline{\mathbb{F}}$, $\textsc{fl}(x) = x(1+\delta)$ with $|\delta|\le \mathrm{u}$.
\end{corollary}


\setlength\fboxsep{2pt}
\section{Arithmetic Operations}
\label{Sec: 1-Ari-Ope}
Let $\mathbb{F} = \mathbb{F}(t, e_{\min}, e_{\max})$ be a given floating point number system and we consider the basic binary operation $\circ\in \{ +, -, *, /\}$ on $\mathbb{F}$, to represent the outcome in $\mathbb{F}$, a straightforward realization is to define the binary operation $\boxed{\circ}$ as the following (for the case of division, we assume $y\neq 0$):
$$x \,\boxed{\circ}\, y := \textsc{fl}(x\circ y),$$
then for any $x, y\in\mathbb{F}$, if $x\circ y\in \overline{\mathbb{F}}$, then $x \,\boxed{\circ}\, y = (x\circ y)(1 + \delta)$ with $|\delta| \le \mathrm{u}$ from the Corollary~\ref{Cor: 1-Rel-err}.

\begin{remark}[Cancellation error]
\label{Rem: 1-Can-err}
    If $x, y\in\bbR$, then the relative error from the following binary operation $\textsc{fl}(x)\,\boxed{+}\,\textsc{fl}(y)$ can be estimated by
    \begin{equation*}
    \begin{aligned}
    \frac{|\textsc{fl}(x)\,\boxed{+}\,\textsc{fl}(y)- (x +  y) |}{|x + y|} &\le \frac{|\textsc{fl}(x)\,\boxed{+}\,\textsc{fl}(y) - (\textsc{fl}(x) +  \textsc{fl}(y)) |}{|x + y|} + \frac{| (\textsc{fl}(x) +  \textsc{fl}(y)) - (x+y)   |}{|x+y|}\\  
    &\le\mathrm{u} + (\mathrm{u} +\mathrm{u}^2) \frac{|x|+|y|}{|x+y|}.
    \end{aligned}
    \end{equation*}
    When $x$ and $y$ are close in magnitude but with opposite signs, the cancellation error will be significant.
\end{remark}
\subsection{Error Accumulation: Multiplication}
\label{SSec: 1-Err-Acc-Mul}
For complicated computations on modern computers, the errors from arithmetic operations will accumulate towards the final result (we do not consider techniques such as fused multiply-add (FMA) here). To quantify the accumulation effect, we will need the following lemma.

\begin{lemma}
\label{Lem: 1-Acc}
    For real numbers $a_1, a_2, \dots, a_n$ with $|a_k|\le \delta$ for $k=1,\dots, n$, then for $n\delta < 1$, the following holds
    $$\prod_{k=1}^n (1 + a_k) = 1 + b_n,$$
    where $|b_n| \le \frac{n\delta}{1 - n\delta}$.
\end{lemma}


\begin{proof}
    The proof is quite easy with induction. When $n=1$, $|b_1| = |a_1|\le \delta \le \frac{\delta}{1 - \delta}$. Suppose the claim holds for $n = m$, then for $n = m +1$, we could see that
    \begin{equation*}
        \prod_{k=1}^{m+1} (1 + a_k) = (1 + b_m) (1 + a_{m+1}) = 1 + b_{m+1}, 
    \end{equation*}
    which implies that $b_{m+1} = b_m + a_{m+1} + a_{m+1}b_m$, with the given bounds on $a_{m+1}$ and $b_m$, we can estimate
    \begin{equation*}
        |b_{m+1}| = | b_m + a_{m+1} + a_{m+1}b_m | \le \frac{(m+1)\delta}{1 - m\delta} \le \frac{(m+1)\delta}{1 - (m+1)\delta}.
    \end{equation*}
\end{proof}


Next, we consider the naive floating point product $P_n$ of $n$ real numbers $\{x_j\}_{j=1}^n \subset \bbR$ with assumption that $(2n-1)\textup{u} < 1$ by the following iteration
\begin{equation*}
    \begin{cases}
        P_k = \fl{x_1} & k=1,\\
        P_k = P_{k-1}~\boxed{\ast}~ \fl{x_k} & k\ge 2.
    \end{cases}
\end{equation*}
Let $\fl{x_k} = x_k(1 + \tau_k)$, then $|\tau_k|\le \textup{u}$. From the $n$-th iteration step $$P_n = P_{n-1}~\boxed{\ast}~ \fl{x_k} = \fl{\fl{P_{n-1}}\fl{x_n}} = \fl{P_{n-1}}\fl{x_n}(1 + \delta_n)$$ such that $|\delta_n|\le \textup{u}$, since $P_{n-1}\in \mathbb{F}$, $\fl{P_{n-1}} = P_{n-1}$, then 
\begin{equation*}
    \begin{aligned}
        P_n &= P_{n-1} \fl{x_n}(1 + \delta_n) = P_{n-2} \fl{x_{n-1}}(1 + \delta_{n-1}) (1 + \delta_k) = \cdots \\
        &=  \fl{x_1} \fl{x_2}\cdots \fl{x_n} \prod_{j=2}^n (1 + \delta_j) = \prod_{j=1}^n x_j (1 + \tau_j)  \prod_{j=2}^n (1 + \delta_j) \\
        &\le  (1 + \eta_n) \prod_{j=1}^n x_j,
    \end{aligned}
\end{equation*}
where $|\eta_n|\le \frac{(2n-1)\textup{u}}{1 - (2n-1)\textup{u}}$ by Lemma~\ref{Lem: 1-Acc}. 

\subsection{Error Accumulation: Addition}
\label{SSec: 1-Err-Acc-Add}
For naive floating point summation $S_n$ of $n$ real numbers $\{x_j \}_{j=1}^n$ by the iteration 
\begin{equation*}
    \begin{cases}
        S_k = 0 & k=0,\\
        S_k = S_{k-1}~\boxed{+}~ \fl{x_k} & k\ge 1,
    \end{cases}
\end{equation*}
we can carry out a similar analysis. Let $S^{\ast}_j = \sum_{k=1}^j x_k$ and  $\fl{x_k} = x_k ( 1 + \tau_k)$ for $|\tau_k|\le \textup{u}$, denote $\Delta S_j = S_j^{\ast} - S_j$, then
\begin{equation*}
    \begin{aligned}
        \Delta S_j &= S_j^{\ast} - S_j = S_{j}^{\ast} - ( S_{j-1}~\boxed{+}~ \fl{x_j}) \\
        &= \Delta S_{j-1}(1 + \delta_j) - \delta_j S_{j}^{\ast} - x_j\tau_j(1+\delta_j),
    \end{aligned}
\end{equation*}
where $|\tau_j|, |\delta_j|\le \textup{u}$. Therefore
\begin{equation*}
    \begin{aligned}
        |\Delta S_n|&\le |\Delta S_{n-1}|(1 + \textup{u}) + \textup{u} \sum_{k=1}^n |x_k| + |x_n| \textup{u}(1 + \textup{u}) \\
        &\le |\Delta S_{n-2}|(1 + \textup{u})^2 + \textup{u}(1+\textup{u})  \sum_{k=1}^j |x_k| + |x_j| \textup{u}(1 + \textup{u})^2 \\
        &\quad +  \textup{u} \sum_{k=1}^{j-1} |x_k| + |x_{j-1}| \textup{u}(1 + \textup{u})\\
        &\le  \cdots \\
        &\le  \sum_{l=1}^{n} \left( \textup{u}(1 + \textup{u})^{l-1} 
 \sum_{k=1}^l |x_k| + |x_{l}|\text{u}(1 + \textup{u})^{l-1} \right) \\
        & \le \sum_{l=1}^{n} \left( \textup{u}(1 + \textup{u})^{l-1} \sum_{k=1}^n |x_k|  \right) + \left(\sum_{l=1}^n |x_{l}|\right)\text{u}(1 + \textup{u})^{n-1}\\
        &= \left(\sum_{l=1}^n |x_{l}|\right) ((1 + \textup{u})^n - 1).
        \end{aligned}
\end{equation*}
Using the previous Lemma~\ref{Lem: 1-Acc} will provide an estimate of $ ((1 + \textup{u})^n - 1)$ as long as $n\textup{u} < 1$.

\section{Exercises}
\label{Sec: Exe-1}
Assume $n\in \bbN$ and $n\textup{u} < 1$, let $\{x_j\}_{j=1}^n$ be a sequence of real numbers, $\textsc{fl}:\overline{\mathbb{F}}\mapsto \mathbb{F}$ is the rounding operation, $\textup{u}$ is the machine epsilon. In the following, we briefly discuss the rounding error for floating point summation (sum reduction). 

\begin{definition}
    A reduction $\Pi$ of the floating point summation 
    $$\fl{x_1} ~\boxed{+}~ \fl{x_2} ~\boxed{+}~\cdots ~\boxed{+}~ \fl{x_n}$$ is an evaluation order for the $~\boxed{+}~$ operations. For example, $n = 4$, then the reduction $\Pi = (3,1,2)$ is corresponding to the following calculation
$$\fl{x_1} ~\boxed{+}~ \fl{x_2} + \underbrace{(\fl{x_3} ~\boxed{+}~ \fl{x_4})}_{=y_1}\to \underbrace{(\fl{x_1} ~\boxed{+}~ \fl{x_2})}_{=y_2} ~\boxed{+}~ y_1 \to \underbrace{(y_2 ~\boxed{+}~ y_1)}_{=y_3},$$
where $y_i$ denotes the result of $i$-th $\boxed{+}$ operation in the reduction. The final summation will be $y_{n-1}$. We denote $T_n(\Pi)$ as the result of floating point summation with reduction order $\Pi$.
\end{definition}


\subsection{Theoretical Part}
\begin{problem}
    Prove that the native summation has the following error estimate
    \begin{equation}
    |T_n(\Pi) - S_n|\le \left( \frac{\textup{u} n }{1 - \textup{u} n } \right)\sum_{j=1}^n |x_j|,
    \end{equation}
    where $S_n = \sum_{j=1}^n x_j$ and $\Pi = (1,2,\dots, (n-1))$.
\end{problem}

\begin{problem}
    Prove that
    \begin{equation}
    \min_{\Pi}|T_n(\Pi) - S_n| \le \left(\frac{ \textup{u} H }{1 - \textup{u} H  }\right)\sum_{j=1}^n |x_j|,
    \end{equation}
    where $H = \lceil\log_2 n \rceil$ and $T_n(\Pi)$ is the floating point summation with reduction order $\Pi$.
\end{problem}

\begin{problem}[Horner's scheme]
\label{Prb: 1-Theo-3}
The evaluation of polynomial 
    $$p(x) = a_0 + a_1 x + \dots + a_n x^n$$
is mostly using Horner's scheme, which writes the polynomial in `nested' form:
\begin{equation}
p(x) = a_0 + x (a_1 + x(a_2 + \dots x(a_{n-1} + x(a_n)))).
\end{equation}                                
Please find an upper bound of the rounding error for this scheme.         
\end{problem}

\subsection{Computational Part}

\begin{problem}[Pairwise summation]
    Based on the theoretical part, implement an algorithm for the summation $\sum_{j=1}^n x_j$ which has $\cO(\textup{u}\log_2 n)$ rounding error.
\end{problem}

\begin{problem}[Kahan compensated summation] 
\label{Prb: 1-Kahan-Sum}
Suppose $a, b\in\mathbb{R}$, the rounding error for the sum $s = \fl{\fl{a}+\fl{b}}$ ($a\ge b$) can be computed using 
    $$\fl{\fl{s - \fl{a}}-\fl{b}}$$
    Based on this property, one can keep track of the rounding error. 
\begin{algorithm}[!htb]
    \SetAlgoLined
    \caption{Kahan compensated summation}\label{alg:1}
    \KwData{$\{x_j\}_{j=1}^n\subset \mathbb{R}$}
    \KwResult{$s_n = \sum_{j=1}^n x_j$ }
    $j\gets 1$, $e_j \gets 0$, $s_j \gets x_j$ //    initialization; 
    \\
    \While{$j < n$}{
        $j\gets j + 1$\\
        $y_j = x_j - e_{j-1}$ //remove compensated error; \\ 
        $s_j = s_{j-1} + y_j$ //perform summation;\\
        $e_j = (s_j - s_{j-1}) - y_j$ //restore the rounding error;
    }
    \label{ALG: KAHAN}
\end{algorithm}
    Implement the Algorithm~\ref{ALG: KAHAN} described below and compare the accuracy with naive summation and pairwise summation with test cases.
\end{problem}


\begin{problem}
    Suppose the inputs $\{x_j\}_{j=1}^n\subset \mathbb{R}$ are randomly distributed (say $x_j\sim U(0,1)$ i.i.d), what is the growth of the expected rounding error with respect to total number of inputs $n$ for the naive summation and pairwise summation? Please explain your result. You can use Algorithm~\ref{ALG: KAHAN} as the accurate result approximately. 
\end{problem}
\newpage
% \section{Extended Reading}
\nocite{higham1993accuracy,higham2019new,muller2006elementary}
\bibliographystyle{apalike}
\bibliography{chap1}
